{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2021025,"sourceType":"datasetVersion","datasetId":1209633},{"sourceId":9073122,"sourceType":"datasetVersion","datasetId":5472989}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Breakhis-400x Dataset","metadata":{}},{"cell_type":"code","source":"print(\"Welcome\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T07:30:39.763414Z","iopub.execute_input":"2025-02-24T07:30:39.764230Z","iopub.status.idle":"2025-02-24T07:30:39.768977Z","shell.execute_reply.started":"2025-02-24T07:30:39.764199Z","shell.execute_reply":"2025-02-24T07:30:39.767904Z"}},"outputs":[{"name":"stdout","text":"Welcome\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"# Final DNBCD Model (Custom CNN + DenseNet + Transfer Learning)","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nimport numpy as np\nimport cv2  \nimport tensorflow as tf\nimport shutil\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import DenseNet121\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# Define paths\ndataset_path = '/kaggle/input/breakhis-400x/Breakhis-400x'\nbase_dir = '/kaggle/working/split_data'\ntrain_dir = os.path.join(base_dir, 'train')\nval_dir = os.path.join(base_dir, 'validation')\ntest_dir = os.path.join(base_dir, 'test')\n\n# Ensure clean split directories\nfor dir_path in [train_dir, val_dir, test_dir]:\n    if os.path.exists(dir_path):\n        shutil.rmtree(dir_path)\n    os.makedirs(dir_path)\n\n# Split dataset into 70% train, 20% test, 10% validation\nall_images = []\nfor class_name in os.listdir(dataset_path):\n    class_dir = os.path.join(dataset_path, class_name)\n    images = [os.path.join(class_dir, img) for img in os.listdir(class_dir)]\n    all_images.extend([(img, class_name) for img in images])\n\ntest_split = 0.2\nval_split = 0.1\ntrain_images, temp_images = train_test_split(\n    all_images,\n    test_size=(test_split + val_split),\n    stratify=[label for _, label in all_images],\n    random_state=42\n)\nval_images, test_images = train_test_split(\n    temp_images,\n    test_size=(test_split / (test_split + val_split)),\n    stratify=[label for _, label in temp_images],\n    random_state=42\n)\n\n# Copy files into respective directories without preprocessing\ndef copy_images(images, target_dir):\n    for img_path, label in images:\n        label_dir = os.path.join(target_dir, label)\n        os.makedirs(label_dir, exist_ok=True)\n        shutil.copy(img_path, label_dir)\n\ncopy_images(train_images, train_dir)\ncopy_images(val_images, val_dir)\ncopy_images(test_images, test_dir)\n\n# Parameters\ninput_shape = (128, 128, 3)\nbatch_size = 32\nepochs = 20\n\n# Create data generators for binary classification\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\nval_datagen = ImageDataGenerator(rescale=1./255)\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='binary'\n)\n\nvalidation_generator = val_datagen.flow_from_directory(\n    val_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='binary'\n)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='binary',\n    shuffle=False\n)\n\n# Print the number of samples in each split\nprint(f\"Training samples: {train_generator.samples}\")\nprint(f\"Validation samples: {validation_generator.samples}\")\nprint(f\"Test samples: {test_generator.samples}\")\n\n# Compute class weights\ntrain_labels = train_generator.classes\nclass_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(train_labels),\n    y=train_labels\n)\nclass_weights = dict(enumerate(class_weights))\nprint(\"Class Weights:\", class_weights)\n\n# Load pre-trained DenseNet121 model + higher-level layers\nbase_model = DenseNet121(weights='imagenet', include_top=False, input_shape=input_shape)\n\n# Freeze the base model\nbase_model.trainable = False\n\n# Add custom layers on top of the base model\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(512, activation='relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(128, activation='relu')(x)\nx = Dropout(0.5)(x)\noutput = Dense(1, activation='sigmoid')(x)\n\n# Create the complete model\nmodel = Model(inputs=base_model.input, outputs=output)\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n\n# Start profiling\ntf.keras.backend.clear_session()\ntf.profiler.experimental.start('logdir/profile')\n\nstart_time = time.time()\nhistory = model.fit(\n    train_generator,\n    epochs=epochs,\n    validation_data=validation_generator,\n    class_weight=class_weights\n)\nprint(\"Initial training time: {:.2f} seconds\".format(time.time() - start_time))\n\n# Stop profiling\ntf.profiler.experimental.stop()\n\n# Unfreeze the base model for fine-tuning\nbase_model.trainable = True\nmodel.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n\n# Fine-tune the model with class weights\nstart_time = time.time()\nhistory_fine = model.fit(\n    train_generator,\n    epochs=epochs,\n    validation_data=validation_generator,\n    class_weight=class_weights\n)\nprint(\"Fine-tuning training time: {:.2f} seconds\".format(time.time() - start_time))\n\n# Evaluate the model on the test set\nstart_time = time.time()\nloss, accuracy = model.evaluate(test_generator)\nprint(\"Test evaluation time: {:.2f} seconds\".format(time.time() - start_time))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T05:28:49.501201Z","iopub.execute_input":"2025-02-24T05:28:49.501464Z","iopub.status.idle":"2025-02-24T05:37:53.061446Z","shell.execute_reply.started":"2025-02-24T05:28:49.501439Z","shell.execute_reply":"2025-02-24T05:37:53.060508Z"}},"outputs":[{"name":"stdout","text":"Found 1273 images belonging to 2 classes.\nFound 182 images belonging to 2 classes.\nFound 365 images belonging to 2 classes.\nTraining samples: 1273\nValidation samples: 182\nTest samples: 365\nClass Weights: {0: 1.548661800486618, 1: 0.738399071925754}\nEpoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 613ms/step - accuracy: 0.5810 - loss: 0.8738 - val_accuracy: 0.7308 - val_loss: 0.5869\nEpoch 2/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 153ms/step - accuracy: 0.6151 - loss: 0.6572 - val_accuracy: 0.7692 - val_loss: 0.5484\nEpoch 3/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 151ms/step - accuracy: 0.6915 - loss: 0.5700 - val_accuracy: 0.8352 - val_loss: 0.4990\nEpoch 4/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 152ms/step - accuracy: 0.6846 - loss: 0.5764 - val_accuracy: 0.7747 - val_loss: 0.5046\nEpoch 5/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 151ms/step - accuracy: 0.7456 - loss: 0.5273 - val_accuracy: 0.7253 - val_loss: 0.5273\nEpoch 6/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 153ms/step - accuracy: 0.7743 - loss: 0.4579 - val_accuracy: 0.7088 - val_loss: 0.5098\nEpoch 7/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 152ms/step - accuracy: 0.7736 - loss: 0.4680 - val_accuracy: 0.8352 - val_loss: 0.4286\nEpoch 8/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 152ms/step - accuracy: 0.7832 - loss: 0.4674 - val_accuracy: 0.8132 - val_loss: 0.4252\nEpoch 9/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 153ms/step - accuracy: 0.7974 - loss: 0.4363 - val_accuracy: 0.8736 - val_loss: 0.3675\nEpoch 10/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 156ms/step - accuracy: 0.8277 - loss: 0.3992 - val_accuracy: 0.8462 - val_loss: 0.3476\nEpoch 11/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 151ms/step - accuracy: 0.8192 - loss: 0.3943 - val_accuracy: 0.8462 - val_loss: 0.3751\nEpoch 12/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 152ms/step - accuracy: 0.8278 - loss: 0.4064 - val_accuracy: 0.8462 - val_loss: 0.3533\nEpoch 13/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 152ms/step - accuracy: 0.8170 - loss: 0.4065 - val_accuracy: 0.8571 - val_loss: 0.3225\nEpoch 14/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 153ms/step - accuracy: 0.8328 - loss: 0.3799 - val_accuracy: 0.8516 - val_loss: 0.3100\nEpoch 15/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 151ms/step - accuracy: 0.8326 - loss: 0.4589 - val_accuracy: 0.8681 - val_loss: 0.3327\nEpoch 16/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 151ms/step - accuracy: 0.8304 - loss: 0.3665 - val_accuracy: 0.8407 - val_loss: 0.3691\nEpoch 17/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 152ms/step - accuracy: 0.8104 - loss: 0.4021 - val_accuracy: 0.8242 - val_loss: 0.3688\nEpoch 18/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 152ms/step - accuracy: 0.8310 - loss: 0.3744 - val_accuracy: 0.8571 - val_loss: 0.3247\nEpoch 19/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 155ms/step - accuracy: 0.8425 - loss: 0.3595 - val_accuracy: 0.8791 - val_loss: 0.2965\nEpoch 20/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 153ms/step - accuracy: 0.8424 - loss: 0.3745 - val_accuracy: 0.8736 - val_loss: 0.3189\nInitial training time: 184.84 seconds\nEpoch 1/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 2s/step - accuracy: 0.7551 - loss: 0.5928 - val_accuracy: 0.7308 - val_loss: 0.7568\nEpoch 2/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 161ms/step - accuracy: 0.8758 - loss: 0.3095 - val_accuracy: 0.6758 - val_loss: 1.3925\nEpoch 3/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 170ms/step - accuracy: 0.9241 - loss: 0.1876 - val_accuracy: 0.6813 - val_loss: 1.6425\nEpoch 4/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 164ms/step - accuracy: 0.9399 - loss: 0.1551 - val_accuracy: 0.6758 - val_loss: 1.7646\nEpoch 5/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 162ms/step - accuracy: 0.9392 - loss: 0.1453 - val_accuracy: 0.6978 - val_loss: 1.4941\nEpoch 6/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 164ms/step - accuracy: 0.9654 - loss: 0.1014 - val_accuracy: 0.7308 - val_loss: 1.1353\nEpoch 7/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 170ms/step - accuracy: 0.9759 - loss: 0.0831 - val_accuracy: 0.8022 - val_loss: 0.6374\nEpoch 8/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 165ms/step - accuracy: 0.9676 - loss: 0.1070 - val_accuracy: 0.8791 - val_loss: 0.4319\nEpoch 9/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 164ms/step - accuracy: 0.9730 - loss: 0.0919 - val_accuracy: 0.8901 - val_loss: 0.3556\nEpoch 10/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 163ms/step - accuracy: 0.9704 - loss: 0.0747 - val_accuracy: 0.9341 - val_loss: 0.1407\nEpoch 11/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 171ms/step - accuracy: 0.9838 - loss: 0.0584 - val_accuracy: 0.9176 - val_loss: 0.1996\nEpoch 12/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 166ms/step - accuracy: 0.9777 - loss: 0.0501 - val_accuracy: 0.9341 - val_loss: 0.1918\nEpoch 13/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 165ms/step - accuracy: 0.9683 - loss: 0.0993 - val_accuracy: 0.9341 - val_loss: 0.2115\nEpoch 14/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 164ms/step - accuracy: 0.9657 - loss: 0.0715 - val_accuracy: 0.9396 - val_loss: 0.2168\nEpoch 15/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 166ms/step - accuracy: 0.9787 - loss: 0.0366 - val_accuracy: 0.9451 - val_loss: 0.2337\nEpoch 16/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 164ms/step - accuracy: 0.9855 - loss: 0.0454 - val_accuracy: 0.9451 - val_loss: 0.1899\nEpoch 17/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 166ms/step - accuracy: 0.9871 - loss: 0.0474 - val_accuracy: 0.9560 - val_loss: 0.1652\nEpoch 18/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 169ms/step - accuracy: 0.9937 - loss: 0.0315 - val_accuracy: 0.9615 - val_loss: 0.1932\nEpoch 19/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 171ms/step - accuracy: 0.9900 - loss: 0.0282 - val_accuracy: 0.9505 - val_loss: 0.1370\nEpoch 20/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 162ms/step - accuracy: 0.9851 - loss: 0.0357 - val_accuracy: 0.9505 - val_loss: 0.1506\nFine-tuning training time: 335.47 seconds\n\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 385ms/step - accuracy: 0.9388 - loss: 0.1776\nTest evaluation time: 4.48 seconds\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# Mobilenet+transfer","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nimport shutil\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import MobileNet\nfrom sklearn.utils.class_weight import compute_class_weight\nimport time\n\n# Define paths\ndataset_path = '/kaggle/input/breakhis-400x/Breakhis-400x'\nbase_dir = '/kaggle/working/split_data'\ntrain_dir = os.path.join(base_dir, 'train')\nval_dir = os.path.join(base_dir, 'validation')\ntest_dir = os.path.join(base_dir, 'test')\n\n# Ensure clean split directories\nfor dir_path in [train_dir, val_dir, test_dir]:\n    if os.path.exists(dir_path):\n        shutil.rmtree(dir_path)\n    os.makedirs(dir_path)\n\n# Split dataset into 70% train, 20% test, 10% validation\nall_images = []\nfor class_name in os.listdir(dataset_path):\n    class_dir = os.path.join(dataset_path, class_name)\n    images = [os.path.join(class_dir, img) for img in os.listdir(class_dir)]\n    all_images.extend([(img, class_name) for img in images])\n\ntest_split = 0.2\nval_split = 0.1\ntrain_images, temp_images = train_test_split(\n    all_images,\n    test_size=(test_split + val_split),\n    stratify=[label for _, label in all_images],\n    random_state=42\n)\nval_images, test_images = train_test_split(\n    temp_images,\n    test_size=(test_split / (test_split + val_split)),\n    stratify=[label for _, label in temp_images],\n    random_state=42\n)\n\n# Copy files into respective directories without preprocessing\ndef copy_images(images, target_dir):\n    for img_path, label in images:\n        label_dir = os.path.join(target_dir, label)\n        os.makedirs(label_dir, exist_ok=True)\n        shutil.copy(img_path, label_dir)\n\ncopy_images(train_images, train_dir)\ncopy_images(val_images, val_dir)\ncopy_images(test_images, test_dir)\n\n# Parameters\ninput_shape = (128, 128, 3)\nbatch_size = 32\nepochs = 20\n\n# Create data generators for binary classification\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\nval_datagen = ImageDataGenerator(rescale=1./255)\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='binary'\n)\n\nvalidation_generator = val_datagen.flow_from_directory(\n    val_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='binary'\n)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='binary',\n    shuffle=False\n)\n\n# Print the number of samples in each split\nprint(f\"Training samples: {train_generator.samples}\")\nprint(f\"Validation samples: {validation_generator.samples}\")\nprint(f\"Test samples: {test_generator.samples}\")\n\n# Compute class weights\ntrain_labels = train_generator.classes\nclass_weights_array = compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(train_labels),\n    y=train_labels\n)\nclass_weights = dict(enumerate(class_weights_array))\nprint(\"Class Weights:\", class_weights)\n\n# Load pre-trained MobileNet model + higher-level layers\nbase_model = MobileNet(weights='imagenet', include_top=False, input_shape=input_shape)\n\n# Freeze the base model\nbase_model.trainable = False\n\n# Add custom layers on top of the base model\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(512, activation='relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(128, activation='relu')(x)\nx = Dropout(0.5)(x)\noutput = Dense(1, activation='sigmoid')(x)\n\n# Create the complete model\nmobilenet_model = Model(inputs=base_model.input, outputs=output)\n\n# Compile the model\nmobilenet_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n\n# Start profiling for initial training\nstart_time = time.time()\nmobilenet_history_initial = mobilenet_model.fit(\n    train_generator,\n    epochs=epochs,\n    validation_data=validation_generator,\n    class_weight=class_weights\n)\nprint(\"Initial training time: {:.2f} seconds\".format(time.time() - start_time))\n\n# Unfreeze the base model for fine-tuning\nbase_model.trainable = True\nmobilenet_model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n\n# Start profiling for fine-tuning\nstart_time = time.time()\nmobilenet_history_fine = mobilenet_model.fit(\n    train_generator,\n    epochs=epochs,\n    validation_data=validation_generator,\n    class_weight=class_weights\n)\nprint(\"Fine-tuning training time: {:.2f} seconds\".format(time.time() - start_time))\n\n# Evaluate the model on the test set\nstart_time = time.time()\nloss, accuracy = mobilenet_model.evaluate(test_generator)\nprint(\"Test evaluation time: {:.2f} seconds\".format(time.time() - start_time))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T05:37:53.063013Z","iopub.execute_input":"2025-02-24T05:37:53.063279Z","iopub.status.idle":"2025-02-24T05:43:29.872688Z","shell.execute_reply.started":"2025-02-24T05:37:53.063254Z","shell.execute_reply":"2025-02-24T05:43:29.871924Z"}},"outputs":[{"name":"stdout","text":"Found 1273 images belonging to 2 classes.\nFound 182 images belonging to 2 classes.\nFound 365 images belonging to 2 classes.\nTraining samples: 1273\nValidation samples: 182\nTest samples: 365\nClass Weights: {0: 1.548661800486618, 1: 0.738399071925754}\nEpoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 289ms/step - accuracy: 0.5467 - loss: 0.9591 - val_accuracy: 0.7802 - val_loss: 0.5379\nEpoch 2/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 145ms/step - accuracy: 0.6478 - loss: 0.6576 - val_accuracy: 0.7253 - val_loss: 0.5639\nEpoch 3/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 145ms/step - accuracy: 0.7003 - loss: 0.5895 - val_accuracy: 0.7857 - val_loss: 0.4818\nEpoch 4/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 145ms/step - accuracy: 0.7511 - loss: 0.5283 - val_accuracy: 0.8132 - val_loss: 0.4663\nEpoch 5/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 148ms/step - accuracy: 0.7358 - loss: 0.5232 - val_accuracy: 0.8022 - val_loss: 0.4646\nEpoch 6/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 144ms/step - accuracy: 0.7635 - loss: 0.5155 - val_accuracy: 0.8242 - val_loss: 0.4310\nEpoch 7/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 147ms/step - accuracy: 0.7875 - loss: 0.4410 - val_accuracy: 0.8407 - val_loss: 0.3927\nEpoch 8/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 145ms/step - accuracy: 0.7984 - loss: 0.4411 - val_accuracy: 0.8407 - val_loss: 0.3959\nEpoch 9/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 146ms/step - accuracy: 0.8034 - loss: 0.4127 - val_accuracy: 0.8407 - val_loss: 0.3945\nEpoch 10/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 144ms/step - accuracy: 0.8158 - loss: 0.3843 - val_accuracy: 0.8352 - val_loss: 0.3478\nEpoch 11/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 143ms/step - accuracy: 0.7896 - loss: 0.4394 - val_accuracy: 0.8571 - val_loss: 0.3335\nEpoch 12/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 145ms/step - accuracy: 0.8226 - loss: 0.3675 - val_accuracy: 0.8242 - val_loss: 0.3917\nEpoch 13/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 145ms/step - accuracy: 0.8179 - loss: 0.3823 - val_accuracy: 0.8242 - val_loss: 0.3653\nEpoch 14/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 147ms/step - accuracy: 0.8383 - loss: 0.3628 - val_accuracy: 0.8516 - val_loss: 0.3082\nEpoch 15/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 145ms/step - accuracy: 0.8001 - loss: 0.4282 - val_accuracy: 0.8626 - val_loss: 0.3414\nEpoch 16/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 144ms/step - accuracy: 0.8255 - loss: 0.3702 - val_accuracy: 0.8626 - val_loss: 0.3347\nEpoch 17/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 142ms/step - accuracy: 0.8592 - loss: 0.3189 - val_accuracy: 0.8626 - val_loss: 0.3023\nEpoch 18/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 143ms/step - accuracy: 0.8565 - loss: 0.3233 - val_accuracy: 0.8681 - val_loss: 0.3145\nEpoch 19/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 145ms/step - accuracy: 0.8693 - loss: 0.2922 - val_accuracy: 0.8626 - val_loss: 0.3079\nEpoch 20/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 143ms/step - accuracy: 0.8422 - loss: 0.3261 - val_accuracy: 0.8626 - val_loss: 0.2966\nInitial training time: 155.14 seconds\nEpoch 1/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 459ms/step - accuracy: 0.7968 - loss: 1.3996 - val_accuracy: 0.6758 - val_loss: 2.9206\nEpoch 2/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 149ms/step - accuracy: 0.8538 - loss: 0.4551 - val_accuracy: 0.7363 - val_loss: 1.1773\nEpoch 3/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 155ms/step - accuracy: 0.8927 - loss: 0.2540 - val_accuracy: 0.8846 - val_loss: 0.3265\nEpoch 4/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 152ms/step - accuracy: 0.9051 - loss: 0.2448 - val_accuracy: 0.9011 - val_loss: 0.2777\nEpoch 5/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 151ms/step - accuracy: 0.9249 - loss: 0.1570 - val_accuracy: 0.9066 - val_loss: 0.2486\nEpoch 6/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 152ms/step - accuracy: 0.9180 - loss: 0.2454 - val_accuracy: 0.8132 - val_loss: 0.4948\nEpoch 7/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 154ms/step - accuracy: 0.9403 - loss: 0.1998 - val_accuracy: 0.8407 - val_loss: 0.4592\nEpoch 8/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 150ms/step - accuracy: 0.9538 - loss: 0.1313 - val_accuracy: 0.8901 - val_loss: 0.2928\nEpoch 9/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 147ms/step - accuracy: 0.9552 - loss: 0.1111 - val_accuracy: 0.9176 - val_loss: 0.2659\nEpoch 10/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 150ms/step - accuracy: 0.9558 - loss: 0.1030 - val_accuracy: 0.9121 - val_loss: 0.2667\nEpoch 11/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 150ms/step - accuracy: 0.9653 - loss: 0.0894 - val_accuracy: 0.8956 - val_loss: 0.3459\nEpoch 12/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 148ms/step - accuracy: 0.9520 - loss: 0.1055 - val_accuracy: 0.9176 - val_loss: 0.2666\nEpoch 13/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 149ms/step - accuracy: 0.9648 - loss: 0.0890 - val_accuracy: 0.8791 - val_loss: 0.3575\nEpoch 14/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 149ms/step - accuracy: 0.9665 - loss: 0.0782 - val_accuracy: 0.9066 - val_loss: 0.3262\nEpoch 15/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 149ms/step - accuracy: 0.9760 - loss: 0.0625 - val_accuracy: 0.9286 - val_loss: 0.2092\nEpoch 16/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 151ms/step - accuracy: 0.9676 - loss: 0.0699 - val_accuracy: 0.9505 - val_loss: 0.2350\nEpoch 17/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 148ms/step - accuracy: 0.9741 - loss: 0.0694 - val_accuracy: 0.9341 - val_loss: 0.2074\nEpoch 18/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 149ms/step - accuracy: 0.9747 - loss: 0.0686 - val_accuracy: 0.9066 - val_loss: 0.2570\nEpoch 19/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 148ms/step - accuracy: 0.9805 - loss: 0.0751 - val_accuracy: 0.8736 - val_loss: 0.3777\nEpoch 20/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 150ms/step - accuracy: 0.9825 - loss: 0.0497 - val_accuracy: 0.9011 - val_loss: 0.3218\nFine-tuning training time: 177.32 seconds\n\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - accuracy: 0.9421 - loss: 0.2617\nTest evaluation time: 1.53 seconds\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# Resnet50+transfer","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2  # OpenCV for general use (not CLAHE)\nimport tensorflow as tf\nimport shutil\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import ResNet50\nfrom sklearn.utils.class_weight import compute_class_weight\nimport time\n\n# Optional: For reproducibility\nimport random\nrandom.seed(42)\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# Define paths\ndataset_path = '/kaggle/input/breakhis-400x/Breakhis-400x'\nbase_dir = '/kaggle/working/split_data'\ntrain_dir = os.path.join(base_dir, 'train')\nval_dir = os.path.join(base_dir, 'validation')\ntest_dir = os.path.join(base_dir, 'test')\n\n# Ensure clean split directories\nfor dir_path in [train_dir, val_dir, test_dir]:\n    if os.path.exists(dir_path):\n        shutil.rmtree(dir_path)\n    os.makedirs(dir_path)\n\n# Split dataset into 70% train, 20% test, 10% validation\nall_images = []\nfor class_name in os.listdir(dataset_path):\n    class_dir = os.path.join(dataset_path, class_name)\n    images = [os.path.join(class_dir, img) for img in os.listdir(class_dir)]\n    all_images.extend([(img, class_name) for img in images])\n\ntest_split = 0.2\nval_split = 0.1\ntrain_images, temp_images = train_test_split(\n    all_images,\n    test_size=(test_split + val_split),\n    stratify=[label for _, label in all_images],\n    random_state=42\n)\nval_images, test_images = train_test_split(\n    temp_images,\n    test_size=(test_split / (test_split + val_split)),\n    stratify=[label for _, label in temp_images],\n    random_state=42\n)\n\n# Copy files into respective directories without preprocessing\ndef copy_images(images, target_dir):\n    for img_path, label in images:\n        label_dir = os.path.join(target_dir, label)\n        os.makedirs(label_dir, exist_ok=True)\n        shutil.copy(img_path, label_dir)  # Copy the original image without preprocessing\n\ncopy_images(train_images, train_dir)\ncopy_images(val_images, val_dir)\ncopy_images(test_images, test_dir)\n\n# Parameters\ninput_shape = (128, 128, 3)\nbatch_size = 32\nepochs = 20\n\n# Create data generators for binary classification\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\nval_datagen = ImageDataGenerator(rescale=1./255)\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='binary'  # Binary classification\n)\n\nvalidation_generator = val_datagen.flow_from_directory(\n    val_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='binary'  # Binary classification\n)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='binary',  # Binary classification\n    shuffle=False\n)\n\n# Print the number of samples in each split\nprint(f\"Training samples: {train_generator.samples}\")\nprint(f\"Validation samples: {validation_generator.samples}\")\nprint(f\"Test samples: {test_generator.samples}\")\n\n# Compute class weights\ntrain_labels = train_generator.classes  # Class indices for training samples\nclass_weights_array = compute_class_weight(\n    class_weight='balanced',  # Balance classes based on their frequency\n    classes=np.unique(train_labels),\n    y=train_labels\n)\nclass_weights = dict(enumerate(class_weights_array))\n\n# Print the calculated class weights\nprint(\"Class Weights:\", class_weights)\n\n# Load pre-trained ResNet50 model + higher-level layers\nbase_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n\n# Freeze the base model\nbase_model.trainable = False\n\n# Add custom layers on top of the base model\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(512, activation='relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(128, activation='relu')(x)\nx = Dropout(0.5)(x)\noutput = Dense(1, activation='sigmoid')(x)  # Binary classification\n\n# Create the complete model\nresnet50_model = Model(inputs=base_model.input, outputs=output)\n\n# Compile the model\nresnet50_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n\n# Start profiling for initial training\nstart_time = time.time()\nresnet50_history_initial = resnet50_model.fit(\n    train_generator,\n    epochs=epochs,\n    validation_data=validation_generator,\n    class_weight=class_weights  # Use calculated class weights\n)\nprint(\"Initial training time: {:.2f} seconds\".format(time.time() - start_time))\n\n# Unfreeze the base model for fine-tuning\nbase_model.trainable = True\nresnet50_model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])  # Reduce learning rate for fine-tuning\n\n# Start profiling for fine-tuning\nstart_time = time.time()\nresnet50_history_fine = resnet50_model.fit(\n    train_generator,\n    epochs=epochs,\n    validation_data=validation_generator,\n    class_weight=class_weights\n)\nprint(\"Fine-tuning training time: {:.2f} seconds\".format(time.time() - start_time))\n\n# Evaluate the model on the test set\nstart_time = time.time()\nloss, accuracy = resnet50_model.evaluate(test_generator)\nprint(\"Test evaluation time: {:.2f} seconds\".format(time.time() - start_time))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T05:43:29.874129Z","iopub.execute_input":"2025-02-24T05:43:29.874420Z","iopub.status.idle":"2025-02-24T05:50:03.764153Z","shell.execute_reply.started":"2025-02-24T05:43:29.874391Z","shell.execute_reply":"2025-02-24T05:50:03.763264Z"}},"outputs":[{"name":"stdout","text":"Found 1273 images belonging to 2 classes.\nFound 182 images belonging to 2 classes.\nFound 365 images belonging to 2 classes.\nTraining samples: 1273\nValidation samples: 182\nTest samples: 365\nClass Weights: {0: 1.548661800486618, 1: 0.738399071925754}\nEpoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 375ms/step - accuracy: 0.4884 - loss: 0.8163 - val_accuracy: 0.3626 - val_loss: 0.7000\nEpoch 2/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 149ms/step - accuracy: 0.4526 - loss: 0.7211 - val_accuracy: 0.6758 - val_loss: 0.6818\nEpoch 3/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 150ms/step - accuracy: 0.5077 - loss: 0.7001 - val_accuracy: 0.6758 - val_loss: 0.6751\nEpoch 4/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 147ms/step - accuracy: 0.4753 - loss: 0.7141 - val_accuracy: 0.6758 - val_loss: 0.6775\nEpoch 5/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 147ms/step - accuracy: 0.5590 - loss: 0.6924 - val_accuracy: 0.6758 - val_loss: 0.6831\nEpoch 6/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 149ms/step - accuracy: 0.5059 - loss: 0.6997 - val_accuracy: 0.6758 - val_loss: 0.6850\nEpoch 7/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 151ms/step - accuracy: 0.5632 - loss: 0.6840 - val_accuracy: 0.4451 - val_loss: 0.6931\nEpoch 8/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 148ms/step - accuracy: 0.5249 - loss: 0.6889 - val_accuracy: 0.3297 - val_loss: 0.7043\nEpoch 9/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 149ms/step - accuracy: 0.4556 - loss: 0.6946 - val_accuracy: 0.6758 - val_loss: 0.6799\nEpoch 10/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 153ms/step - accuracy: 0.5495 - loss: 0.6933 - val_accuracy: 0.5275 - val_loss: 0.6920\nEpoch 11/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 148ms/step - accuracy: 0.5273 - loss: 0.6973 - val_accuracy: 0.6758 - val_loss: 0.6765\nEpoch 12/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 147ms/step - accuracy: 0.5709 - loss: 0.6923 - val_accuracy: 0.3297 - val_loss: 0.7058\nEpoch 13/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 147ms/step - accuracy: 0.4776 - loss: 0.6947 - val_accuracy: 0.6758 - val_loss: 0.6764\nEpoch 14/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 151ms/step - accuracy: 0.5361 - loss: 0.7041 - val_accuracy: 0.6758 - val_loss: 0.6763\nEpoch 15/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 152ms/step - accuracy: 0.5571 - loss: 0.7024 - val_accuracy: 0.6099 - val_loss: 0.6781\nEpoch 16/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 147ms/step - accuracy: 0.5401 - loss: 0.6917 - val_accuracy: 0.4176 - val_loss: 0.6993\nEpoch 17/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 149ms/step - accuracy: 0.5251 - loss: 0.6773 - val_accuracy: 0.3407 - val_loss: 0.7089\nEpoch 18/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 151ms/step - accuracy: 0.4972 - loss: 0.6895 - val_accuracy: 0.4670 - val_loss: 0.6892\nEpoch 19/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 154ms/step - accuracy: 0.5168 - loss: 0.6937 - val_accuracy: 0.4451 - val_loss: 0.6917\nEpoch 20/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 149ms/step - accuracy: 0.5517 - loss: 0.6956 - val_accuracy: 0.4341 - val_loss: 0.6967\nInitial training time: 163.66 seconds\nEpoch 1/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 744ms/step - accuracy: 0.7005 - loss: 2.1896 - val_accuracy: 0.6758 - val_loss: 0.8528\nEpoch 2/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 166ms/step - accuracy: 0.8094 - loss: 0.4229 - val_accuracy: 0.6758 - val_loss: 1.5046\nEpoch 3/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 162ms/step - accuracy: 0.8718 - loss: 0.3010 - val_accuracy: 0.6758 - val_loss: 1.7050\nEpoch 4/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.9044 - loss: 0.2392 - val_accuracy: 0.6758 - val_loss: 4.6884\nEpoch 5/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 161ms/step - accuracy: 0.9255 - loss: 0.1796 - val_accuracy: 0.6758 - val_loss: 17.0128\nEpoch 6/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 164ms/step - accuracy: 0.9254 - loss: 0.2170 - val_accuracy: 0.6758 - val_loss: 8.8651\nEpoch 7/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.9411 - loss: 0.1779 - val_accuracy: 0.6758 - val_loss: 14.1576\nEpoch 8/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.9560 - loss: 0.1177 - val_accuracy: 0.6758 - val_loss: 10.0654\nEpoch 9/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.9757 - loss: 0.0878 - val_accuracy: 0.6758 - val_loss: 13.3198\nEpoch 10/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 163ms/step - accuracy: 0.9652 - loss: 0.0954 - val_accuracy: 0.6758 - val_loss: 3.2581\nEpoch 11/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 162ms/step - accuracy: 0.9593 - loss: 0.0971 - val_accuracy: 0.6758 - val_loss: 4.2592\nEpoch 12/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.9594 - loss: 0.0949 - val_accuracy: 0.6758 - val_loss: 1.9097\nEpoch 13/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.9835 - loss: 0.0675 - val_accuracy: 0.6758 - val_loss: 5.5465\nEpoch 14/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 161ms/step - accuracy: 0.9739 - loss: 0.0944 - val_accuracy: 0.6374 - val_loss: 0.8177\nEpoch 15/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 163ms/step - accuracy: 0.9798 - loss: 0.0669 - val_accuracy: 0.6758 - val_loss: 1.6448\nEpoch 16/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 162ms/step - accuracy: 0.9712 - loss: 0.0738 - val_accuracy: 0.6538 - val_loss: 2.3595\nEpoch 17/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 161ms/step - accuracy: 0.9815 - loss: 0.0488 - val_accuracy: 0.6758 - val_loss: 1.7211\nEpoch 18/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 163ms/step - accuracy: 0.9793 - loss: 0.0615 - val_accuracy: 0.5769 - val_loss: 1.6464\nEpoch 19/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 165ms/step - accuracy: 0.9926 - loss: 0.0284 - val_accuracy: 0.6868 - val_loss: 2.6455\nEpoch 20/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 162ms/step - accuracy: 0.9874 - loss: 0.0483 - val_accuracy: 0.6648 - val_loss: 1.5480\nFine-tuning training time: 224.24 seconds\n\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 173ms/step - accuracy: 0.4464 - loss: 2.5244\nTest evaluation time: 2.15 seconds\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# Vgg19+transfer","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2  # OpenCV for general use (not CLAHE)\nimport tensorflow as tf\nimport shutil\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.applications.vgg19 import preprocess_input\nfrom sklearn.utils.class_weight import compute_class_weight\nimport time\n\n# Optional: For reproducibility\nimport random\nrandom.seed(42)\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# Define paths\ndataset_path = '/kaggle/input/breakhis-400x/Breakhis-400x'\nbase_dir = '/kaggle/working/split_data'\ntrain_dir = os.path.join(base_dir, 'train')\nval_dir = os.path.join(base_dir, 'validation')\ntest_dir = os.path.join(base_dir, 'test')\n\n# Ensure clean split directories\nfor dir_path in [train_dir, val_dir, test_dir]:\n    if os.path.exists(dir_path):\n        shutil.rmtree(dir_path)\n    os.makedirs(dir_path)\n\n# Split dataset into 70% train, 20% test, 10% validation\nall_images = []\nfor class_name in os.listdir(dataset_path):\n    class_dir = os.path.join(dataset_path, class_name)\n    images = [os.path.join(class_dir, img) for img in os.listdir(class_dir)]\n    all_images.extend([(img, class_name) for img in images])\n\ntest_split = 0.2\nval_split = 0.1\ntrain_images, temp_images = train_test_split(\n    all_images,\n    test_size=(test_split + val_split),\n    stratify=[label for _, label in all_images],\n    random_state=42\n)\nval_images, test_images = train_test_split(\n    temp_images,\n    test_size=(test_split / (test_split + val_split)),\n    stratify=[label for _, label in temp_images],\n    random_state=42\n)\n\n# Copy files into respective directories without preprocessing\ndef copy_images(images, target_dir):\n    for img_path, label in images:\n        label_dir = os.path.join(target_dir, label)\n        os.makedirs(label_dir, exist_ok=True)\n        shutil.copy(img_path, label_dir)  # Copy the original image without preprocessing\n\ncopy_images(train_images, train_dir)\ncopy_images(val_images, val_dir)\ncopy_images(test_images, test_dir)\n\n# Parameters\ninput_shape = (128, 128, 3)\nbatch_size = 32\nepochs = 20\n\n# Create data generators for binary classification\ntrain_datagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input,  # VGG19-specific preprocessing\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\nval_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\ntest_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='binary'  # Binary classification\n)\n\nvalidation_generator = val_datagen.flow_from_directory(\n    val_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='binary'  # Binary classification\n)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='binary',  # Binary classification\n    shuffle=False\n)\n\n# Print the number of samples in each split\nprint(f\"Training samples: {train_generator.samples}\")\nprint(f\"Validation samples: {validation_generator.samples}\")\nprint(f\"Test samples: {test_generator.samples}\")\n\n# Compute class weights\ntrain_labels = train_generator.classes  # Class indices for training samples\nclass_weights_array = compute_class_weight(\n    class_weight='balanced',  # Balance classes based on their frequency\n    classes=np.unique(train_labels),\n    y=train_labels\n)\nclass_weights = dict(enumerate(class_weights_array))\n\n# Print the calculated class weights\nprint(\"Class Weights:\", class_weights)\n\n# Load pre-trained VGG19 model + higher-level layers\nbase_model = VGG19(weights='imagenet', include_top=False, input_shape=input_shape)\n\n# Freeze the base model\nbase_model.trainable = False\n\n# Add custom layers on top of the base model\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(512, activation='relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(128, activation='relu')(x)\nx = Dropout(0.5)(x)\noutput = Dense(1, activation='sigmoid')(x)  # Binary classification\n\n# Create the complete model\nvgg19_model = Model(inputs=base_model.input, outputs=output)\n\n# Compile the model\nvgg19_model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n\n# Start profiling for initial training\nstart_time = time.time()\nvgg19_history_initial = vgg19_model.fit(\n    train_generator,\n    epochs=epochs,\n    validation_data=validation_generator,\n    class_weight=class_weights  # Use calculated class weights\n)\nprint(\"Initial training time: {:.2f} seconds\".format(time.time() - start_time))\n\n# Unfreeze the base model for fine-tuning\nbase_model.trainable = True\nvgg19_model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])  # Reduce learning rate for fine-tuning\n\n# Start profiling for fine-tuning\nstart_time = time.time()\nvgg19_history_fine = vgg19_model.fit(\n    train_generator,\n    epochs=epochs,\n    validation_data=validation_generator,\n    class_weight=class_weights\n)\nprint(\"Fine-tuning training time: {:.2f} seconds\".format(time.time() - start_time))\n\n# Evaluate the model on the test set\nstart_time = time.time()\nloss, accuracy = vgg19_model.evaluate(test_generator)\nprint(\"Test evaluation time: {:.2f} seconds\".format(time.time() - start_time))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T05:50:03.766983Z","iopub.execute_input":"2025-02-24T05:50:03.767322Z","iopub.status.idle":"2025-02-24T05:55:44.073129Z","shell.execute_reply.started":"2025-02-24T05:50:03.767294Z","shell.execute_reply":"2025-02-24T05:55:44.072191Z"}},"outputs":[{"name":"stdout","text":"Found 1273 images belonging to 2 classes.\nFound 182 images belonging to 2 classes.\nFound 365 images belonging to 2 classes.\nTraining samples: 1273\nValidation samples: 182\nTest samples: 365\nClass Weights: {0: 1.548661800486618, 1: 0.738399071925754}\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m80134624/80134624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nEpoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 377ms/step - accuracy: 0.6274 - loss: 2.2554 - val_accuracy: 0.7747 - val_loss: 0.4400\nEpoch 2/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 149ms/step - accuracy: 0.6718 - loss: 1.0267 - val_accuracy: 0.7967 - val_loss: 0.4257\nEpoch 3/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 151ms/step - accuracy: 0.7366 - loss: 0.6772 - val_accuracy: 0.8187 - val_loss: 0.4377\nEpoch 4/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 155ms/step - accuracy: 0.7331 - loss: 0.5968 - val_accuracy: 0.8462 - val_loss: 0.4593\nEpoch 5/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 151ms/step - accuracy: 0.7292 - loss: 0.5976 - val_accuracy: 0.8681 - val_loss: 0.4286\nEpoch 6/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 152ms/step - accuracy: 0.7429 - loss: 0.5755 - val_accuracy: 0.8571 - val_loss: 0.3998\nEpoch 7/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 148ms/step - accuracy: 0.7842 - loss: 0.4884 - val_accuracy: 0.7857 - val_loss: 0.4881\nEpoch 8/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 149ms/step - accuracy: 0.7828 - loss: 0.5221 - val_accuracy: 0.8352 - val_loss: 0.3677\nEpoch 9/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 148ms/step - accuracy: 0.7960 - loss: 0.4990 - val_accuracy: 0.8681 - val_loss: 0.4096\nEpoch 10/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 151ms/step - accuracy: 0.7975 - loss: 0.4735 - val_accuracy: 0.8132 - val_loss: 0.4388\nEpoch 11/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 151ms/step - accuracy: 0.8014 - loss: 0.4714 - val_accuracy: 0.8571 - val_loss: 0.3643\nEpoch 12/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 148ms/step - accuracy: 0.7835 - loss: 0.4886 - val_accuracy: 0.8516 - val_loss: 0.3765\nEpoch 13/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 152ms/step - accuracy: 0.8066 - loss: 0.4552 - val_accuracy: 0.8407 - val_loss: 0.3954\nEpoch 14/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 149ms/step - accuracy: 0.8074 - loss: 0.4579 - val_accuracy: 0.8626 - val_loss: 0.3287\nEpoch 15/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 150ms/step - accuracy: 0.8104 - loss: 0.4718 - val_accuracy: 0.8681 - val_loss: 0.3320\nEpoch 16/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 147ms/step - accuracy: 0.8297 - loss: 0.3914 - val_accuracy: 0.8407 - val_loss: 0.3537\nEpoch 17/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 152ms/step - accuracy: 0.8481 - loss: 0.3683 - val_accuracy: 0.8626 - val_loss: 0.3430\nEpoch 18/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 149ms/step - accuracy: 0.8314 - loss: 0.4222 - val_accuracy: 0.8626 - val_loss: 0.3210\nEpoch 19/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 148ms/step - accuracy: 0.8111 - loss: 0.4290 - val_accuracy: 0.8681 - val_loss: 0.3123\nEpoch 20/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 148ms/step - accuracy: 0.8156 - loss: 0.4306 - val_accuracy: 0.8571 - val_loss: 0.3288\nInitial training time: 160.45 seconds\nEpoch 1/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 362ms/step - accuracy: 0.6378 - loss: 1.0925 - val_accuracy: 0.7967 - val_loss: 0.5490\nEpoch 2/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 161ms/step - accuracy: 0.7339 - loss: 0.5534 - val_accuracy: 0.6099 - val_loss: 0.6590\nEpoch 3/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 164ms/step - accuracy: 0.7244 - loss: 0.5361 - val_accuracy: 0.8187 - val_loss: 0.4855\nEpoch 4/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 159ms/step - accuracy: 0.8332 - loss: 0.4625 - val_accuracy: 0.8407 - val_loss: 0.5066\nEpoch 5/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 162ms/step - accuracy: 0.8610 - loss: 0.4266 - val_accuracy: 0.8407 - val_loss: 0.4215\nEpoch 6/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.8766 - loss: 0.3898 - val_accuracy: 0.7857 - val_loss: 0.4832\nEpoch 7/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 163ms/step - accuracy: 0.8654 - loss: 0.3398 - val_accuracy: 0.8681 - val_loss: 0.3515\nEpoch 8/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.8833 - loss: 0.3201 - val_accuracy: 0.8571 - val_loss: 0.3214\nEpoch 9/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 162ms/step - accuracy: 0.8948 - loss: 0.3012 - val_accuracy: 0.8022 - val_loss: 0.4745\nEpoch 10/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 161ms/step - accuracy: 0.9146 - loss: 0.2536 - val_accuracy: 0.8901 - val_loss: 0.2470\nEpoch 11/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 162ms/step - accuracy: 0.9040 - loss: 0.2324 - val_accuracy: 0.8132 - val_loss: 0.3579\nEpoch 12/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 164ms/step - accuracy: 0.8935 - loss: 0.2618 - val_accuracy: 0.8407 - val_loss: 0.3395\nEpoch 13/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 162ms/step - accuracy: 0.8909 - loss: 0.2329 - val_accuracy: 0.8846 - val_loss: 0.2938\nEpoch 14/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 162ms/step - accuracy: 0.8893 - loss: 0.3244 - val_accuracy: 0.9011 - val_loss: 0.3084\nEpoch 15/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.9275 - loss: 0.1957 - val_accuracy: 0.9066 - val_loss: 0.2452\nEpoch 16/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 162ms/step - accuracy: 0.9338 - loss: 0.1843 - val_accuracy: 0.9011 - val_loss: 0.2444\nEpoch 17/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 162ms/step - accuracy: 0.9220 - loss: 0.2131 - val_accuracy: 0.9011 - val_loss: 0.2394\nEpoch 18/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 161ms/step - accuracy: 0.9091 - loss: 0.2132 - val_accuracy: 0.9066 - val_loss: 0.2599\nEpoch 19/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 160ms/step - accuracy: 0.8689 - loss: 0.2680 - val_accuracy: 0.8736 - val_loss: 0.2987\nEpoch 20/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 163ms/step - accuracy: 0.9209 - loss: 0.1952 - val_accuracy: 0.8407 - val_loss: 0.3755\nFine-tuning training time: 174.02 seconds\n\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 223ms/step - accuracy: 0.9244 - loss: 0.1930\nTest evaluation time: 2.72 seconds\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# DenseNet121","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport shutil\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import DenseNet121\nfrom sklearn.utils.class_weight import compute_class_weight\nimport time\n\n# Define paths\ndataset_path = '/kaggle/input/breakhis-400x/Breakhis-400x'\nbase_dir = '/kaggle/working/split_data'\ntrain_dir = os.path.join(base_dir, 'train')\nval_dir = os.path.join(base_dir, 'validation')\ntest_dir = os.path.join(base_dir, 'test')\n\n# Ensure clean split directories\nfor dir_path in [train_dir, val_dir, test_dir]:\n    if os.path.exists(dir_path):\n        shutil.rmtree(dir_path)\n    os.makedirs(dir_path)\n\n# Split dataset into 70% train, 20% test, 10% validation\nall_images = []\nfor class_name in os.listdir(dataset_path):\n    class_dir = os.path.join(dataset_path, class_name)\n    images = [os.path.join(class_dir, img) for img in os.listdir(class_dir)]\n    all_images.extend([(img, class_name) for img in images])\n\ntest_split = 0.2\nval_split = 0.1\ntrain_images, temp_images = train_test_split(\n    all_images,\n    test_size=(test_split + val_split),\n    stratify=[label for _, label in all_images],\n    random_state=42\n)\nval_images, test_images = train_test_split(\n    temp_images,\n    test_size=(test_split / (test_split + val_split)),\n    stratify=[label for _, label in temp_images],\n    random_state=42\n)\n\n# Copy files into respective directories without preprocessing\ndef copy_images(images, target_dir):\n    for img_path, label in images:\n        label_dir = os.path.join(target_dir, label)\n        os.makedirs(label_dir, exist_ok=True)\n        shutil.copy(img_path, label_dir)\n\ncopy_images(train_images, train_dir)\ncopy_images(val_images, val_dir)\ncopy_images(test_images, test_dir)\n\n# Parameters\ninput_shape = (128, 128, 3)\nbatch_size = 32\nepochs = 20\n\n# Create data generators for binary classification\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\nval_datagen = ImageDataGenerator(rescale=1./255)\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='binary'\n)\n\nvalidation_generator = val_datagen.flow_from_directory(\n    val_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='binary'\n)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='binary',\n    shuffle=False\n)\n\n# Print the number of samples in each split\nprint(f\"Training samples: {train_generator.samples}\")\nprint(f\"Validation samples: {validation_generator.samples}\")\nprint(f\"Test samples: {test_generator.samples}\")\n\n# Compute class weights\ntrain_labels = train_generator.classes\nclass_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(train_labels),\n    y=train_labels\n)\nclass_weights = dict(enumerate(class_weights))\n\n# Print the calculated class weights\nprint(\"Class Weights:\", class_weights)\n\n# Load pre-trained DenseNet121 model + higher-level layers\nbase_model = DenseNet121(weights='imagenet', include_top=False, input_shape=input_shape)\n\n# Freeze the base model\nbase_model.trainable = False\n\n# Add Global Average Pooling and output layer\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\noutput = Dense(1, activation='sigmoid')(x)  # Binary classification output\n\n# Create the complete model\nmodel_densenet = Model(inputs=base_model.input, outputs=output)\n\n# Compile the model\nmodel_densenet.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n\n# Start profiling for training\nstart_time = time.time()\nhistory_densenet = model_densenet.fit(\n    train_generator,\n    epochs=epochs,\n    validation_data=validation_generator,\n    class_weight=class_weights\n)\nprint(\"Training time: {:.2f} seconds\".format(time.time() - start_time))\n\n# Evaluate the model on the test set\nstart_time = time.time()\nloss, accuracy = model_densenet.evaluate(test_generator)\nprint(\"Test evaluation time: {:.2f} seconds\".format(time.time() - start_time))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T05:55:44.074602Z","iopub.execute_input":"2025-02-24T05:55:44.074915Z","iopub.status.idle":"2025-02-24T05:58:49.795621Z","shell.execute_reply.started":"2025-02-24T05:55:44.074887Z","shell.execute_reply":"2025-02-24T05:58:49.794750Z"}},"outputs":[{"name":"stdout","text":"Found 1273 images belonging to 2 classes.\nFound 182 images belonging to 2 classes.\nFound 365 images belonging to 2 classes.\nTraining samples: 1273\nValidation samples: 182\nTest samples: 365\nClass Weights: {0: 1.548661800486618, 1: 0.738399071925754}\nEpoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 557ms/step - accuracy: 0.4976 - loss: 0.8280 - val_accuracy: 0.4011 - val_loss: 0.8081\nEpoch 2/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 152ms/step - accuracy: 0.4722 - loss: 0.7923 - val_accuracy: 0.4505 - val_loss: 0.7837\nEpoch 3/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 154ms/step - accuracy: 0.4908 - loss: 0.7533 - val_accuracy: 0.4670 - val_loss: 0.7662\nEpoch 4/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 148ms/step - accuracy: 0.5108 - loss: 0.7501 - val_accuracy: 0.4451 - val_loss: 0.7641\nEpoch 5/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 147ms/step - accuracy: 0.5296 - loss: 0.7435 - val_accuracy: 0.4835 - val_loss: 0.7372\nEpoch 6/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 148ms/step - accuracy: 0.5470 - loss: 0.7192 - val_accuracy: 0.4835 - val_loss: 0.7233\nEpoch 7/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 153ms/step - accuracy: 0.5771 - loss: 0.7047 - val_accuracy: 0.4890 - val_loss: 0.7093\nEpoch 8/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 147ms/step - accuracy: 0.6144 - loss: 0.6785 - val_accuracy: 0.4780 - val_loss: 0.7166\nEpoch 9/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 149ms/step - accuracy: 0.5890 - loss: 0.6844 - val_accuracy: 0.5549 - val_loss: 0.6881\nEpoch 10/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 150ms/step - accuracy: 0.6167 - loss: 0.6749 - val_accuracy: 0.5275 - val_loss: 0.6880\nEpoch 11/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 149ms/step - accuracy: 0.6013 - loss: 0.6613 - val_accuracy: 0.5275 - val_loss: 0.6841\nEpoch 12/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 150ms/step - accuracy: 0.6225 - loss: 0.6343 - val_accuracy: 0.5385 - val_loss: 0.6663\nEpoch 13/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 149ms/step - accuracy: 0.6138 - loss: 0.6299 - val_accuracy: 0.5714 - val_loss: 0.6507\nEpoch 14/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 148ms/step - accuracy: 0.6496 - loss: 0.6300 - val_accuracy: 0.5604 - val_loss: 0.6564\nEpoch 15/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 149ms/step - accuracy: 0.6836 - loss: 0.5962 - val_accuracy: 0.5934 - val_loss: 0.6372\nEpoch 16/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 153ms/step - accuracy: 0.7127 - loss: 0.5805 - val_accuracy: 0.5934 - val_loss: 0.6334\nEpoch 17/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 151ms/step - accuracy: 0.7185 - loss: 0.5888 - val_accuracy: 0.6209 - val_loss: 0.6285\nEpoch 18/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 149ms/step - accuracy: 0.7152 - loss: 0.5839 - val_accuracy: 0.6209 - val_loss: 0.6216\nEpoch 19/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 150ms/step - accuracy: 0.7166 - loss: 0.5912 - val_accuracy: 0.6264 - val_loss: 0.6171\nEpoch 20/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 153ms/step - accuracy: 0.6971 - loss: 0.5703 - val_accuracy: 0.6319 - val_loss: 0.6172\nTraining time: 176.02 seconds\n\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 432ms/step - accuracy: 0.7211 - loss: 0.5633\nTest evaluation time: 5.02 seconds\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# Mobilenet","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport shutil\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import MobileNet\nfrom sklearn.utils.class_weight import compute_class_weight\nimport time\n\n# Define paths\ndataset_path = '/kaggle/input/breakhis-400x/Breakhis-400x'\nbase_dir = '/kaggle/working/split_data'\ntrain_dir = os.path.join(base_dir, 'train')\nval_dir = os.path.join(base_dir, 'validation')\ntest_dir = os.path.join(base_dir, 'test')\n\n# Ensure clean split directories\nfor dir_path in [train_dir, val_dir, test_dir]:\n    if os.path.exists(dir_path):\n        shutil.rmtree(dir_path)\n    os.makedirs(dir_path)\n\n# Split dataset into 70% train, 20% test, 10% validation\nall_images = []\nfor class_name in os.listdir(dataset_path):\n    class_dir = os.path.join(dataset_path, class_name)\n    images = [os.path.join(class_dir, img) for img in os.listdir(class_dir)]\n    all_images.extend([(img, class_name) for img in images])\n\ntest_split = 0.2\nval_split = 0.1\ntrain_images, temp_images = train_test_split(\n    all_images,\n    test_size=(test_split + val_split),\n    stratify=[label for _, label in all_images],\n    random_state=42\n)\nval_images, test_images = train_test_split(\n    temp_images,\n    test_size=(test_split / (test_split + val_split)),\n    stratify=[label for _, label in temp_images],\n    random_state=42\n)\n\n# Copy files into respective directories without preprocessing\ndef copy_images(images, target_dir):\n    for img_path, label in images:\n        label_dir = os.path.join(target_dir, label)\n        os.makedirs(label_dir, exist_ok=True)\n        shutil.copy(img_path, label_dir)\n\ncopy_images(train_images, train_dir)\ncopy_images(val_images, val_dir)\ncopy_images(test_images, test_dir)\n\n# Parameters\ninput_shape = (128, 128, 3)\nbatch_size = 32\nepochs = 20\n\n# Create data generators for binary classification\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\nval_datagen = ImageDataGenerator(rescale=1./255)\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='binary'\n)\n\nvalidation_generator = val_datagen.flow_from_directory(\n    val_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='binary'\n)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='binary',\n    shuffle=False\n)\n\n# Print the number of samples in each split\nprint(f\"Training samples: {train_generator.samples}\")\nprint(f\"Validation samples: {validation_generator.samples}\")\nprint(f\"Test samples: {test_generator.samples}\")\n\n# Compute class weights\ntrain_labels = train_generator.classes\nclass_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(train_labels),\n    y=train_labels\n)\nclass_weights = dict(enumerate(class_weights))\n\n# Print the calculated class weights\nprint(\"Class Weights:\", class_weights)\n\n# Load pre-trained MobileNet model + higher-level layers\nbase_model = MobileNet(weights='imagenet', include_top=False, input_shape=input_shape)\n\n# Freeze the base model\nbase_model.trainable = False\n\n# Add Global Average Pooling and output layer\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\noutput = Dense(1, activation='sigmoid')(x)  # Binary classification output\n\n# Create the complete model\nmodel_mobilenet = Model(inputs=base_model.input, outputs=output)\n\n# Compile the model\nmodel_mobilenet.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n\n# Start profiling for training\nstart_time = time.time()\nhistory_mobilenet = model_mobilenet.fit(\n    train_generator,\n    epochs=epochs,\n    validation_data=validation_generator,\n    class_weight=class_weights\n)\nprint(\"Training time: {:.2f} seconds\".format(time.time() - start_time))\n\n# Evaluate the model on the test set\nstart_time = time.time()\nloss, accuracy = model_mobilenet.evaluate(test_generator)\nprint(\"Test evaluation time: {:.2f} seconds\".format(time.time() - start_time))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T05:58:49.796895Z","iopub.execute_input":"2025-02-24T05:58:49.797174Z","iopub.status.idle":"2025-02-24T06:01:18.180939Z","shell.execute_reply.started":"2025-02-24T05:58:49.797148Z","shell.execute_reply":"2025-02-24T06:01:18.180102Z"}},"outputs":[{"name":"stdout","text":"Found 1273 images belonging to 2 classes.\nFound 182 images belonging to 2 classes.\nFound 365 images belonging to 2 classes.\nTraining samples: 1273\nValidation samples: 182\nTest samples: 365\nClass Weights: {0: 1.548661800486618, 1: 0.738399071925754}\nEpoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 215ms/step - accuracy: 0.5029 - loss: 0.7631 - val_accuracy: 0.4341 - val_loss: 0.8511\nEpoch 2/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 143ms/step - accuracy: 0.5817 - loss: 0.7096 - val_accuracy: 0.4451 - val_loss: 0.8293\nEpoch 3/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 145ms/step - accuracy: 0.5713 - loss: 0.6886 - val_accuracy: 0.4560 - val_loss: 0.8124\nEpoch 4/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 146ms/step - accuracy: 0.6020 - loss: 0.6895 - val_accuracy: 0.4890 - val_loss: 0.7807\nEpoch 5/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 144ms/step - accuracy: 0.6361 - loss: 0.6391 - val_accuracy: 0.5110 - val_loss: 0.7577\nEpoch 6/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 145ms/step - accuracy: 0.6602 - loss: 0.6173 - val_accuracy: 0.5385 - val_loss: 0.7321\nEpoch 7/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 144ms/step - accuracy: 0.6321 - loss: 0.6365 - val_accuracy: 0.5549 - val_loss: 0.7168\nEpoch 8/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 145ms/step - accuracy: 0.6876 - loss: 0.5873 - val_accuracy: 0.5604 - val_loss: 0.7130\nEpoch 9/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 141ms/step - accuracy: 0.6800 - loss: 0.6004 - val_accuracy: 0.5989 - val_loss: 0.6866\nEpoch 10/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 142ms/step - accuracy: 0.6791 - loss: 0.5961 - val_accuracy: 0.6044 - val_loss: 0.6854\nEpoch 11/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 142ms/step - accuracy: 0.6880 - loss: 0.5665 - val_accuracy: 0.5934 - val_loss: 0.6861\nEpoch 12/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 145ms/step - accuracy: 0.6964 - loss: 0.5877 - val_accuracy: 0.6154 - val_loss: 0.6654\nEpoch 13/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 141ms/step - accuracy: 0.6872 - loss: 0.5888 - val_accuracy: 0.6264 - val_loss: 0.6676\nEpoch 14/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 141ms/step - accuracy: 0.7040 - loss: 0.5678 - val_accuracy: 0.6264 - val_loss: 0.6636\nEpoch 15/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 145ms/step - accuracy: 0.7056 - loss: 0.5709 - val_accuracy: 0.6374 - val_loss: 0.6442\nEpoch 16/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 141ms/step - accuracy: 0.7372 - loss: 0.5312 - val_accuracy: 0.6484 - val_loss: 0.6347\nEpoch 17/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 147ms/step - accuracy: 0.7183 - loss: 0.5474 - val_accuracy: 0.6429 - val_loss: 0.6437\nEpoch 18/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 143ms/step - accuracy: 0.7266 - loss: 0.5239 - val_accuracy: 0.6484 - val_loss: 0.6275\nEpoch 19/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 143ms/step - accuracy: 0.7476 - loss: 0.5257 - val_accuracy: 0.6484 - val_loss: 0.6206\nEpoch 20/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 141ms/step - accuracy: 0.7532 - loss: 0.5251 - val_accuracy: 0.6538 - val_loss: 0.6015\nTraining time: 143.95 seconds\n\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 121ms/step - accuracy: 0.7478 - loss: 0.5447\nTest evaluation time: 1.56 seconds\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"# Resnet50","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport shutil\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import ResNet50\nfrom sklearn.utils.class_weight import compute_class_weight\nimport time\n\n# Define paths\ndataset_path = '/kaggle/input/breakhis-400x/Breakhis-400x'\nbase_dir = '/kaggle/working/split_data'\ntrain_dir = os.path.join(base_dir, 'train')\nval_dir = os.path.join(base_dir, 'validation')\ntest_dir = os.path.join(base_dir, 'test')\n\n# Ensure clean split directories\nfor dir_path in [train_dir, val_dir, test_dir]:\n    if os.path.exists(dir_path):\n        shutil.rmtree(dir_path)\n    os.makedirs(dir_path)\n\n# Split dataset into 70% train, 20% test, 10% validation\nall_images = []\nfor class_name in os.listdir(dataset_path):\n    class_dir = os.path.join(dataset_path, class_name)\n    images = [os.path.join(class_dir, img) for img in os.listdir(class_dir)]\n    all_images.extend([(img, class_name) for img in images])\n\ntest_split = 0.2\nval_split = 0.1\ntrain_images, temp_images = train_test_split(\n    all_images,\n    test_size=(test_split + val_split),\n    stratify=[label for _, label in all_images],\n    random_state=42\n)\nval_images, test_images = train_test_split(\n    temp_images,\n    test_size=(test_split / (test_split + val_split)),\n    stratify=[label for _, label in temp_images],\n    random_state=42\n)\n\n# Copy files into respective directories without preprocessing\ndef copy_images(images, target_dir):\n    for img_path, label in images:\n        label_dir = os.path.join(target_dir, label)\n        os.makedirs(label_dir, exist_ok=True)\n        shutil.copy(img_path, label_dir)\n\ncopy_images(train_images, train_dir)\ncopy_images(val_images, val_dir)\ncopy_images(test_images, test_dir)\n\n# Parameters\ninput_shape = (128, 128, 3)\nbatch_size = 32\nepochs = 20\n\n# Create data generators for binary classification\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\nval_datagen = ImageDataGenerator(rescale=1./255)\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='binary'\n)\n\nvalidation_generator = val_datagen.flow_from_directory(\n    val_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='binary'\n)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='binary',\n    shuffle=False\n)\n\n# Print the number of samples in each split\nprint(f\"Training samples: {train_generator.samples}\")\nprint(f\"Validation samples: {validation_generator.samples}\")\nprint(f\"Test samples: {test_generator.samples}\")\n\n# Compute class weights\ntrain_labels = train_generator.classes\nclass_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(train_labels),\n    y=train_labels\n)\nclass_weights = dict(enumerate(class_weights))\n\n# Print the calculated class weights\nprint(\"Class Weights:\", class_weights)\n\n# Load pre-trained ResNet50 model + higher-level layers\nbase_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n\n# Freeze the base model\nbase_model.trainable = False\n\n# Add Global Average Pooling and output layer\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\noutput = Dense(1, activation='sigmoid')(x)  # Binary classification output\n\n# Create the complete model\nmodel_resnet = Model(inputs=base_model.input, outputs=output)\n\n# Compile the model\nmodel_resnet.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n\n# Start profiling for training\nstart_time = time.time()\nhistory_resnet = model_resnet.fit(\n    train_generator,\n    epochs=epochs,\n    validation_data=validation_generator,\n    class_weight=class_weights\n)\nprint(\"Training time: {:.2f} seconds\".format(time.time() - start_time))\n\n# Evaluate the model on the test set\nstart_time = time.time()\nloss, accuracy = model_resnet.evaluate(test_generator)\nprint(\"Test evaluation time: {:.2f} seconds\".format(time.time() - start_time))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T06:01:18.182028Z","iopub.execute_input":"2025-02-24T06:01:18.182306Z","iopub.status.idle":"2025-02-24T06:04:00.504200Z","shell.execute_reply.started":"2025-02-24T06:01:18.182279Z","shell.execute_reply":"2025-02-24T06:04:00.503120Z"}},"outputs":[{"name":"stdout","text":"Found 1273 images belonging to 2 classes.\nFound 182 images belonging to 2 classes.\nFound 365 images belonging to 2 classes.\nTraining samples: 1273\nValidation samples: 182\nTest samples: 365\nClass Weights: {0: 1.548661800486618, 1: 0.738399071925754}\nEpoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 289ms/step - accuracy: 0.3442 - loss: 0.7059 - val_accuracy: 0.3352 - val_loss: 0.7025\nEpoch 2/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 145ms/step - accuracy: 0.3311 - loss: 0.6992 - val_accuracy: 0.3571 - val_loss: 0.6969\nEpoch 3/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 144ms/step - accuracy: 0.4617 - loss: 0.6837 - val_accuracy: 0.5714 - val_loss: 0.6910\nEpoch 4/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 146ms/step - accuracy: 0.5389 - loss: 0.6878 - val_accuracy: 0.5440 - val_loss: 0.6931\nEpoch 5/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 143ms/step - accuracy: 0.4029 - loss: 0.7021 - val_accuracy: 0.3791 - val_loss: 0.6955\nEpoch 6/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 145ms/step - accuracy: 0.4469 - loss: 0.6853 - val_accuracy: 0.5769 - val_loss: 0.6921\nEpoch 7/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 148ms/step - accuracy: 0.5410 - loss: 0.6868 - val_accuracy: 0.6044 - val_loss: 0.6917\nEpoch 8/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 146ms/step - accuracy: 0.4985 - loss: 0.6916 - val_accuracy: 0.6044 - val_loss: 0.6912\nEpoch 9/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 146ms/step - accuracy: 0.4290 - loss: 0.7034 - val_accuracy: 0.5440 - val_loss: 0.6925\nEpoch 10/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 144ms/step - accuracy: 0.5149 - loss: 0.6899 - val_accuracy: 0.6374 - val_loss: 0.6895\nEpoch 11/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 145ms/step - accuracy: 0.5380 - loss: 0.6964 - val_accuracy: 0.6099 - val_loss: 0.6909\nEpoch 12/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 147ms/step - accuracy: 0.5796 - loss: 0.6896 - val_accuracy: 0.6648 - val_loss: 0.6886\nEpoch 13/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 145ms/step - accuracy: 0.5942 - loss: 0.6917 - val_accuracy: 0.6593 - val_loss: 0.6887\nEpoch 14/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 143ms/step - accuracy: 0.6236 - loss: 0.6810 - val_accuracy: 0.6209 - val_loss: 0.6902\nEpoch 15/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 144ms/step - accuracy: 0.4702 - loss: 0.6902 - val_accuracy: 0.5495 - val_loss: 0.6913\nEpoch 16/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 146ms/step - accuracy: 0.4619 - loss: 0.6931 - val_accuracy: 0.4121 - val_loss: 0.6941\nEpoch 17/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 147ms/step - accuracy: 0.4059 - loss: 0.7084 - val_accuracy: 0.5110 - val_loss: 0.6912\nEpoch 18/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 144ms/step - accuracy: 0.4663 - loss: 0.7005 - val_accuracy: 0.6374 - val_loss: 0.6880\nEpoch 19/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 141ms/step - accuracy: 0.6645 - loss: 0.6863 - val_accuracy: 0.6484 - val_loss: 0.6877\nEpoch 20/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 143ms/step - accuracy: 0.5851 - loss: 0.6905 - val_accuracy: 0.5440 - val_loss: 0.6902\nTraining time: 156.85 seconds\n\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 156ms/step - accuracy: 0.5298 - loss: 0.6918\nTest evaluation time: 1.96 seconds\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Vgg19","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport shutil\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import VGG19\nfrom sklearn.utils.class_weight import compute_class_weight\nimport time\n\n# Define paths\ndataset_path = '/kaggle/input/breakhis-400x/Breakhis-400x'\nbase_dir = '/kaggle/working/split_data'\ntrain_dir = os.path.join(base_dir, 'train')\nval_dir = os.path.join(base_dir, 'validation')\ntest_dir = os.path.join(base_dir, 'test')\n\n# Ensure clean split directories\nfor dir_path in [train_dir, val_dir, test_dir]:\n    if os.path.exists(dir_path):\n        shutil.rmtree(dir_path)\n    os.makedirs(dir_path)\n\n# Split dataset into 70% train, 20% test, 10% validation\nall_images = []\nfor class_name in os.listdir(dataset_path):\n    class_dir = os.path.join(dataset_path, class_name)\n    images = [os.path.join(class_dir, img) for img in os.listdir(class_dir)]\n    all_images.extend([(img, class_name) for img in images])\n\ntest_split = 0.2\nval_split = 0.1\ntrain_images, temp_images = train_test_split(\n    all_images,\n    test_size=(test_split + val_split),\n    stratify=[label for _, label in all_images],\n    random_state=42\n)\nval_images, test_images = train_test_split(\n    temp_images,\n    test_size=(test_split / (test_split + val_split)),\n    stratify=[label for _, label in temp_images],\n    random_state=42\n)\n\n# Copy files into respective directories without preprocessing\ndef copy_images(images, target_dir):\n    for img_path, label in images:\n        label_dir = os.path.join(target_dir, label)\n        os.makedirs(label_dir, exist_ok=True)\n        shutil.copy(img_path, label_dir)\n\ncopy_images(train_images, train_dir)\ncopy_images(val_images, val_dir)\ncopy_images(test_images, test_dir)\n\n# Parameters\ninput_shape = (128, 128, 3)\nbatch_size = 32\nepochs = 20\n\n# Create data generators for binary classification\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\nval_datagen = ImageDataGenerator(rescale=1./255)\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='binary'\n)\n\nvalidation_generator = val_datagen.flow_from_directory(\n    val_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='binary'\n)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='binary',\n    shuffle=False\n)\n\n# Print the number of samples in each split\nprint(f\"Training samples: {train_generator.samples}\")\nprint(f\"Validation samples: {validation_generator.samples}\")\nprint(f\"Test samples: {test_generator.samples}\")\n\n# Compute class weights\ntrain_labels = train_generator.classes\nclass_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(train_labels),\n    y=train_labels\n)\nclass_weights = dict(enumerate(class_weights))\n\n# Print the calculated class weights\nprint(\"Class Weights:\", class_weights)\n\n# Load pre-trained VGG19 model + higher-level layers\nbase_model = VGG19(weights='imagenet', include_top=False, input_shape=input_shape)\n\n# Freeze the base model\nbase_model.trainable = False\n\n# Add Global Average Pooling and output layer\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\noutput = Dense(1, activation='sigmoid')(x)  # Binary classification output\n\n# Create the complete model\nmodel_vgg = Model(inputs=base_model.input, outputs=output)\n\n# Compile the model\nmodel_vgg.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n\n# Start profiling for training\nstart_time = time.time()\nhistory_vgg = model_vgg.fit(\n    train_generator,\n    epochs=epochs,\n    validation_data=validation_generator,\n    class_weight=class_weights\n)\nprint(\"Training time: {:.2f} seconds\".format(time.time() - start_time))\n\n# Evaluate the model on the test set\nstart_time = time.time()\nloss, accuracy = model_vgg.evaluate(test_generator)\nprint(\"Test evaluation time: {:.2f} seconds\".format(time.time() - start_time))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T06:04:00.505879Z","iopub.execute_input":"2025-02-24T06:04:00.506326Z","iopub.status.idle":"2025-02-24T06:06:32.010191Z","shell.execute_reply.started":"2025-02-24T06:04:00.506284Z","shell.execute_reply":"2025-02-24T06:06:32.009302Z"}},"outputs":[{"name":"stdout","text":"Found 1273 images belonging to 2 classes.\nFound 182 images belonging to 2 classes.\nFound 365 images belonging to 2 classes.\nTraining samples: 1273\nValidation samples: 182\nTest samples: 365\nClass Weights: {0: 1.548661800486618, 1: 0.738399071925754}\nEpoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 179ms/step - accuracy: 0.3386 - loss: 0.7143 - val_accuracy: 0.3791 - val_loss: 0.7274\nEpoch 2/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 146ms/step - accuracy: 0.3770 - loss: 0.6950 - val_accuracy: 0.4560 - val_loss: 0.6996\nEpoch 3/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 145ms/step - accuracy: 0.4276 - loss: 0.6946 - val_accuracy: 0.5220 - val_loss: 0.6890\nEpoch 4/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 154ms/step - accuracy: 0.5136 - loss: 0.6800 - val_accuracy: 0.5824 - val_loss: 0.6830\nEpoch 5/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 151ms/step - accuracy: 0.5265 - loss: 0.6822 - val_accuracy: 0.5604 - val_loss: 0.6831\nEpoch 6/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 156ms/step - accuracy: 0.5211 - loss: 0.6774 - val_accuracy: 0.5934 - val_loss: 0.6790\nEpoch 7/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 155ms/step - accuracy: 0.5557 - loss: 0.6756 - val_accuracy: 0.5659 - val_loss: 0.6790\nEpoch 8/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 153ms/step - accuracy: 0.5476 - loss: 0.6825 - val_accuracy: 0.5714 - val_loss: 0.6741\nEpoch 9/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 151ms/step - accuracy: 0.6162 - loss: 0.6747 - val_accuracy: 0.5769 - val_loss: 0.6748\nEpoch 10/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 153ms/step - accuracy: 0.5801 - loss: 0.6792 - val_accuracy: 0.5989 - val_loss: 0.6708\nEpoch 11/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 148ms/step - accuracy: 0.5514 - loss: 0.6930 - val_accuracy: 0.6154 - val_loss: 0.6692\nEpoch 12/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 148ms/step - accuracy: 0.5886 - loss: 0.6873 - val_accuracy: 0.6429 - val_loss: 0.6674\nEpoch 13/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 155ms/step - accuracy: 0.6312 - loss: 0.6677 - val_accuracy: 0.6538 - val_loss: 0.6624\nEpoch 14/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 152ms/step - accuracy: 0.6592 - loss: 0.6627 - val_accuracy: 0.6429 - val_loss: 0.6642\nEpoch 15/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 150ms/step - accuracy: 0.6374 - loss: 0.6640 - val_accuracy: 0.6264 - val_loss: 0.6641\nEpoch 16/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 153ms/step - accuracy: 0.6277 - loss: 0.6779 - val_accuracy: 0.6538 - val_loss: 0.6603\nEpoch 17/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 153ms/step - accuracy: 0.6778 - loss: 0.6495 - val_accuracy: 0.6538 - val_loss: 0.6595\nEpoch 18/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 151ms/step - accuracy: 0.6262 - loss: 0.6493 - val_accuracy: 0.6648 - val_loss: 0.6576\nEpoch 19/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 149ms/step - accuracy: 0.6261 - loss: 0.6642 - val_accuracy: 0.6648 - val_loss: 0.6570\nEpoch 20/20\n\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 150ms/step - accuracy: 0.6642 - loss: 0.6531 - val_accuracy: 0.6813 - val_loss: 0.6531\nTraining time: 147.37 seconds\n\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 87ms/step - accuracy: 0.7321 - loss: 0.6456\nTest evaluation time: 1.23 seconds\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"# BUSI","metadata":{}},{"cell_type":"code","source":"print(\"Welcome\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T06:06:32.011343Z","iopub.execute_input":"2025-02-24T06:06:32.011625Z","iopub.status.idle":"2025-02-24T06:06:32.015940Z","shell.execute_reply.started":"2025-02-24T06:06:32.011598Z","shell.execute_reply":"2025-02-24T06:06:32.015078Z"}},"outputs":[{"name":"stdout","text":"Welcome\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# Final DNBCD Model (Custom CNN + DenseNet + Transfer Learning)","metadata":{}},{"cell_type":"code","source":"import os \nimport numpy as np\nimport cv2  # OpenCV for general use (not CLAHE)\nimport tensorflow as tf\nimport shutil\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import DenseNet121\nfrom sklearn.utils.class_weight import compute_class_weight\nimport time\n\n# Define paths\ndataset_path = '/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT'\nbase_dir = '/kaggle/working/split_data'\ntrain_dir = os.path.join(base_dir, 'train')\nval_dir = os.path.join(base_dir, 'validation')\ntest_dir = os.path.join(base_dir, 'test')\n\n# Ensure clean split directories\nfor dir_path in [train_dir, val_dir, test_dir]:\n    if os.path.exists(dir_path):\n        shutil.rmtree(dir_path)\n    os.makedirs(dir_path)\n\n# Split dataset into 70% train, 20% test, 10% validation\nall_images = []\nfor class_name in os.listdir(dataset_path):\n    class_dir = os.path.join(dataset_path, class_name)\n    images = [os.path.join(class_dir, img) for img in os.listdir(class_dir)]\n    all_images.extend([(img, class_name) for img in images])\n\ntest_split = 0.2\nval_split = 0.1\ntrain_images, temp_images = train_test_split(\n    all_images,\n    test_size=(test_split + val_split),\n    stratify=[label for _, label in all_images],\n    random_state=42\n)\nval_images, test_images = train_test_split(\n    temp_images,\n    test_size=(test_split / (test_split + val_split)),\n    stratify=[label for _, label in temp_images],\n    random_state=42\n)\n\n# Copy files into respective directories without preprocessing\ndef copy_images(images, target_dir):\n    for img_path, label in images:\n        label_dir = os.path.join(target_dir, label)\n        os.makedirs(label_dir, exist_ok=True)\n        shutil.copy(img_path, label_dir)\n\ncopy_images(train_images, train_dir)\ncopy_images(val_images, val_dir)\ncopy_images(test_images, test_dir)\n\n# Parameters\ninput_shape = (128, 128, 3)\nbatch_size = 32\nepochs = 20\n\n# Create data generators for binary classification\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\nval_datagen = ImageDataGenerator(rescale=1./255)\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical'  # Categorical classification\n)\n\nvalidation_generator = val_datagen.flow_from_directory(\n    val_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical'  # Categorical classification\n)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',  # Categorical classification\n    shuffle=False\n)\n\n# Print the number of samples in each split\nprint(f\"Training samples: {train_generator.samples}\")\nprint(f\"Validation samples: {validation_generator.samples}\")\nprint(f\"Test samples: {test_generator.samples}\")\n\n# Compute class weights\ntrain_labels = train_generator.classes\nclass_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(train_labels),\n    y=train_labels\n)\nclass_weights = dict(enumerate(class_weights))\n\n# Print the calculated class weights\nprint(\"Class Weights:\", class_weights)\n\n# Load pre-trained DenseNet121 model + higher-level layers\nbase_model = DenseNet121(weights='imagenet', include_top=False, input_shape=input_shape)\n\n# Freeze the base model\nbase_model.trainable = False\n\n# Add custom layers on top of the base model\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(512, activation='relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(128, activation='relu')(x)\nx = Dropout(0.5)(x)\noutput = Dense(3, activation='softmax')(x)  # Categorical classification\n\n# Create the complete model\nmodel = Model(inputs=base_model.input, outputs=output)\n\n# Compile the model\nmodel.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Start profiling for initial training\nstart_time = time.time()\nhistory = model.fit(\n    train_generator,\n    epochs=epochs,\n    validation_data=validation_generator,\n    class_weight=class_weights\n)\nprint(\"Initial training time: {:.2f} seconds\".format(time.time() - start_time))\n\n# Unfreeze the base model for fine-tuning\nbase_model.trainable = True\nmodel.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])  # Reduce learning rate for fine-tuning\n\n# Start profiling for fine-tuning\nstart_time = time.time()\nhistory_fine = model.fit(\n    train_generator,\n    epochs=epochs,\n    validation_data=validation_generator,\n    class_weight=class_weights\n)\nprint(\"Fine-tuning training time: {:.2f} seconds\".format(time.time() - start_time))\n\n# Evaluate the model on the test set\nstart_time = time.time()\nloss, accuracy = model.evaluate(test_generator)\nprint(\"Test evaluation time: {:.2f} seconds\".format(time.time() - start_time))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T06:06:32.019970Z","iopub.execute_input":"2025-02-24T06:06:32.020205Z","iopub.status.idle":"2025-02-24T06:17:29.672450Z","shell.execute_reply.started":"2025-02-24T06:06:32.020181Z","shell.execute_reply":"2025-02-24T06:17:29.671739Z"}},"outputs":[{"name":"stdout","text":"Found 1104 images belonging to 3 classes.\nFound 158 images belonging to 3 classes.\nFound 316 images belonging to 3 classes.\nTraining samples: 1104\nValidation samples: 158\nTest samples: 316\nClass Weights: {0: 0.5906902086677368, 1: 1.2474576271186442, 2: 1.978494623655914}\nEpoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 901ms/step - accuracy: 0.5155 - loss: 1.3867 - val_accuracy: 0.7278 - val_loss: 0.5754\nEpoch 2/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 216ms/step - accuracy: 0.6981 - loss: 0.6297 - val_accuracy: 0.7722 - val_loss: 0.4591\nEpoch 3/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 219ms/step - accuracy: 0.6795 - loss: 0.6594 - val_accuracy: 0.7785 - val_loss: 0.4500\nEpoch 4/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 214ms/step - accuracy: 0.7620 - loss: 0.5821 - val_accuracy: 0.7848 - val_loss: 0.4537\nEpoch 5/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 218ms/step - accuracy: 0.7395 - loss: 0.5708 - val_accuracy: 0.8228 - val_loss: 0.4159\nEpoch 6/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 217ms/step - accuracy: 0.7647 - loss: 0.4878 - val_accuracy: 0.8101 - val_loss: 0.4073\nEpoch 7/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 218ms/step - accuracy: 0.7634 - loss: 0.5157 - val_accuracy: 0.8228 - val_loss: 0.3852\nEpoch 8/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 224ms/step - accuracy: 0.7602 - loss: 0.4965 - val_accuracy: 0.8228 - val_loss: 0.3727\nEpoch 9/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 219ms/step - accuracy: 0.7602 - loss: 0.4863 - val_accuracy: 0.8418 - val_loss: 0.3759\nEpoch 10/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 224ms/step - accuracy: 0.7875 - loss: 0.4628 - val_accuracy: 0.8354 - val_loss: 0.3643\nEpoch 11/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 226ms/step - accuracy: 0.7836 - loss: 0.4655 - val_accuracy: 0.8038 - val_loss: 0.4161\nEpoch 12/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 225ms/step - accuracy: 0.7895 - loss: 0.4587 - val_accuracy: 0.8481 - val_loss: 0.3483\nEpoch 13/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 222ms/step - accuracy: 0.8151 - loss: 0.4299 - val_accuracy: 0.8544 - val_loss: 0.3742\nEpoch 14/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 224ms/step - accuracy: 0.7587 - loss: 0.4756 - val_accuracy: 0.8228 - val_loss: 0.3718\nEpoch 15/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 223ms/step - accuracy: 0.7832 - loss: 0.4559 - val_accuracy: 0.8544 - val_loss: 0.3557\nEpoch 16/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 220ms/step - accuracy: 0.7943 - loss: 0.4257 - val_accuracy: 0.8418 - val_loss: 0.3722\nEpoch 17/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 215ms/step - accuracy: 0.7808 - loss: 0.4620 - val_accuracy: 0.8481 - val_loss: 0.3606\nEpoch 18/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 220ms/step - accuracy: 0.7970 - loss: 0.4598 - val_accuracy: 0.8228 - val_loss: 0.3884\nEpoch 19/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 219ms/step - accuracy: 0.8044 - loss: 0.4363 - val_accuracy: 0.8291 - val_loss: 0.3862\nEpoch 20/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 217ms/step - accuracy: 0.8309 - loss: 0.3961 - val_accuracy: 0.8228 - val_loss: 0.3743\nInitial training time: 236.45 seconds\nEpoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1740377563.254425     114 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'input_reduce_fusion_1299', 4 bytes spill stores, 4 bytes spill loads\nptxas warning : Registers are spilled to local memory in function 'input_reduce_fusion_1294', 8 bytes spill stores, 8 bytes spill loads\nptxas warning : Registers are spilled to local memory in function 'input_reduce_select_fusion_54', 4 bytes spill stores, 4 bytes spill loads\nptxas warning : Registers are spilled to local memory in function 'input_reduce_select_fusion_47', 4 bytes spill stores, 4 bytes spill loads\nptxas warning : Registers are spilled to local memory in function 'input_reduce_fusion_1040', 4 bytes spill stores, 4 bytes spill loads\n\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m19/35\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 232ms/step - accuracy: 0.7539 - loss: 0.6939","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1740377636.618206     114 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'input_reduce_select_fusion_22', 4 bytes spill stores, 4 bytes spill loads\nptxas warning : Registers are spilled to local memory in function 'input_reduce_select_fusion_19', 4 bytes spill stores, 4 bytes spill loads\nptxas warning : Registers are spilled to local memory in function 'input_reduce_fusion_468', 4 bytes spill stores, 4 bytes spill loads\n\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 3s/step - accuracy: 0.7620 - loss: 0.6308 - val_accuracy: 0.8038 - val_loss: 0.6066\nEpoch 2/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 249ms/step - accuracy: 0.8013 - loss: 0.3868 - val_accuracy: 0.7089 - val_loss: 0.7301\nEpoch 3/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 245ms/step - accuracy: 0.8458 - loss: 0.3540 - val_accuracy: 0.7152 - val_loss: 0.6120\nEpoch 4/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 245ms/step - accuracy: 0.8500 - loss: 0.3476 - val_accuracy: 0.7532 - val_loss: 0.4939\nEpoch 5/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 244ms/step - accuracy: 0.8651 - loss: 0.2732 - val_accuracy: 0.7785 - val_loss: 0.4518\nEpoch 6/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 234ms/step - accuracy: 0.8858 - loss: 0.2727 - val_accuracy: 0.8861 - val_loss: 0.3559\nEpoch 7/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 244ms/step - accuracy: 0.8894 - loss: 0.2888 - val_accuracy: 0.8734 - val_loss: 0.2551\nEpoch 8/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 244ms/step - accuracy: 0.9078 - loss: 0.1887 - val_accuracy: 0.9367 - val_loss: 0.1581\nEpoch 9/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 239ms/step - accuracy: 0.9141 - loss: 0.1939 - val_accuracy: 0.9177 - val_loss: 0.2015\nEpoch 10/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 249ms/step - accuracy: 0.8872 - loss: 0.2441 - val_accuracy: 0.9557 - val_loss: 0.1220\nEpoch 11/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 234ms/step - accuracy: 0.9194 - loss: 0.1808 - val_accuracy: 0.9177 - val_loss: 0.2342\nEpoch 12/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 239ms/step - accuracy: 0.9225 - loss: 0.1887 - val_accuracy: 0.9620 - val_loss: 0.1273\nEpoch 13/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 228ms/step - accuracy: 0.9321 - loss: 0.1839 - val_accuracy: 0.9430 - val_loss: 0.1376\nEpoch 14/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 228ms/step - accuracy: 0.9392 - loss: 0.1598 - val_accuracy: 0.9367 - val_loss: 0.1728\nEpoch 15/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 228ms/step - accuracy: 0.9378 - loss: 0.1864 - val_accuracy: 0.9177 - val_loss: 0.2406\nEpoch 16/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 233ms/step - accuracy: 0.9210 - loss: 0.1936 - val_accuracy: 0.8924 - val_loss: 0.2589\nEpoch 17/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 229ms/step - accuracy: 0.9614 - loss: 0.1023 - val_accuracy: 0.9241 - val_loss: 0.1884\nEpoch 18/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 226ms/step - accuracy: 0.9485 - loss: 0.1332 - val_accuracy: 0.9494 - val_loss: 0.1632\nEpoch 19/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 226ms/step - accuracy: 0.9340 - loss: 0.1765 - val_accuracy: 0.9494 - val_loss: 0.1799\nEpoch 20/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 224ms/step - accuracy: 0.9471 - loss: 0.1285 - val_accuracy: 0.9684 - val_loss: 0.1090\nFine-tuning training time: 397.70 seconds\n\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 887ms/step - accuracy: 0.8978 - loss: 0.3099\nTest evaluation time: 8.44 seconds\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"# Mobilenet+transfer","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2  # OpenCV for general use (not CLAHE)\nimport tensorflow as tf\nimport shutil\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import MobileNet\nfrom sklearn.utils.class_weight import compute_class_weight\nimport time\n\n# Define paths\ndataset_path = '/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT'\nbase_dir = '/kaggle/working/split_data'\ntrain_dir = os.path.join(base_dir, 'train')\nval_dir = os.path.join(base_dir, 'validation')\ntest_dir = os.path.join(base_dir, 'test')\n\n# Ensure clean split directories\nfor dir_path in [train_dir, val_dir, test_dir]:\n    if os.path.exists(dir_path):\n        shutil.rmtree(dir_path)\n    os.makedirs(dir_path)\n\n# Split dataset into 70% train, 20% test, 10% validation\nall_images = []\nfor class_name in os.listdir(dataset_path):\n    class_dir = os.path.join(dataset_path, class_name)\n    images = [os.path.join(class_dir, img) for img in os.listdir(class_dir)]\n    all_images.extend([(img, class_name) for img in images])\n\ntest_split = 0.2\nval_split = 0.1\ntrain_images, temp_images = train_test_split(\n    all_images,\n    test_size=(test_split + val_split),\n    stratify=[label for _, label in all_images],\n    random_state=42\n)\nval_images, test_images = train_test_split(\n    temp_images,\n    test_size=(test_split / (test_split + val_split)),\n    stratify=[label for _, label in temp_images],\n    random_state=42\n)\n\n# Copy files into respective directories without preprocessing\ndef copy_images(images, target_dir):\n    for img_path, label in images:\n        label_dir = os.path.join(target_dir, label)\n        os.makedirs(label_dir, exist_ok=True)\n        shutil.copy(img_path, label_dir)  # Copy the original image without preprocessing\n\ncopy_images(train_images, train_dir)\ncopy_images(val_images, val_dir)\ncopy_images(test_images, test_dir)\n\n# Parameters\ninput_shape = (128, 128, 3)\nbatch_size = 32\nepochs = 20\n\n# Create data generators for classification\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\nval_datagen = ImageDataGenerator(rescale=1./255)\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical'  # Categorical classification\n)\n\nvalidation_generator = val_datagen.flow_from_directory(\n    val_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical'  # Categorical classification\n)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',  # Categorical classification\n    shuffle=False\n)\n\n# Print the number of samples in each split\nprint(f\"Training samples: {train_generator.samples}\")\nprint(f\"Validation samples: {validation_generator.samples}\")\nprint(f\"Test samples: {test_generator.samples}\")\n\n# Compute class weights\ntrain_labels = train_generator.classes  # Class indices for training samples\nclass_weights_array = compute_class_weight(\n    class_weight='balanced',  # Balance classes based on their frequency\n    classes=np.unique(train_labels),\n    y=train_labels\n)\nclass_weights = dict(enumerate(class_weights_array))\n\n# Print the calculated class weights\nprint(\"Class Weights:\", class_weights)\n\n# Load pre-trained MobileNet model + higher-level layers\nbase_model = MobileNet(weights='imagenet', include_top=False, input_shape=input_shape)\n\n# Freeze the base model\nbase_model.trainable = False\n\n# Add custom layers on top of the base model\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(512, activation='relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(128, activation='relu')(x)\nx = Dropout(0.5)(x)\noutput = Dense(3, activation='softmax')(x)  # Categorical classification\n\n# Create the complete model\nmobilenet_model = Model(inputs=base_model.input, outputs=output)\n\n# Compile the model\nmobilenet_model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Start profiling for initial training\nstart_time = time.time()\nmobilenet_history_initial = mobilenet_model.fit(\n    train_generator,\n    epochs=epochs,\n    validation_data=validation_generator,\n    class_weight=class_weights  # Use calculated class weights\n)\nprint(\"Initial training time: {:.2f} seconds\".format(time.time() - start_time))\n\n# Unfreeze the base model for fine-tuning\nbase_model.trainable = True\nmobilenet_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])  # Reduce learning rate for fine-tuning\n\n# Start profiling for fine-tuning\nstart_time = time.time()\nmobilenet_history_fine = mobilenet_model.fit(\n    train_generator,\n    epochs=epochs,\n    validation_data=validation_generator,\n    class_weight=class_weights\n)\nprint(\"Fine-tuning training time: {:.2f} seconds\".format(time.time() - start_time))\n\n# Evaluate the model on the test set\nstart_time = time.time()\nloss, accuracy = mobilenet_model.evaluate(test_generator)\nprint(\"Test evaluation time: {:.2f} seconds\".format(time.time() - start_time))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T06:17:29.674139Z","iopub.execute_input":"2025-02-24T06:17:29.674407Z","iopub.status.idle":"2025-02-24T06:24:25.448415Z","shell.execute_reply.started":"2025-02-24T06:17:29.674381Z","shell.execute_reply":"2025-02-24T06:24:25.447647Z"}},"outputs":[{"name":"stdout","text":"Found 1104 images belonging to 3 classes.\nFound 158 images belonging to 3 classes.\nFound 316 images belonging to 3 classes.\nTraining samples: 1104\nValidation samples: 158\nTest samples: 316\nClass Weights: {0: 0.5906902086677368, 1: 1.2474576271186442, 2: 1.978494623655914}\nEpoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 413ms/step - accuracy: 0.5353 - loss: 1.2023 - val_accuracy: 0.6962 - val_loss: 0.5678\nEpoch 2/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 205ms/step - accuracy: 0.7333 - loss: 0.6353 - val_accuracy: 0.7722 - val_loss: 0.4549\nEpoch 3/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 207ms/step - accuracy: 0.7503 - loss: 0.5380 - val_accuracy: 0.8354 - val_loss: 0.3962\nEpoch 4/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 204ms/step - accuracy: 0.7888 - loss: 0.4761 - val_accuracy: 0.8608 - val_loss: 0.3662\nEpoch 5/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 205ms/step - accuracy: 0.7823 - loss: 0.5259 - val_accuracy: 0.8165 - val_loss: 0.3527\nEpoch 6/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 205ms/step - accuracy: 0.8094 - loss: 0.4077 - val_accuracy: 0.7911 - val_loss: 0.3890\nEpoch 7/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 210ms/step - accuracy: 0.8083 - loss: 0.4422 - val_accuracy: 0.8165 - val_loss: 0.3722\nEpoch 8/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 206ms/step - accuracy: 0.8058 - loss: 0.4462 - val_accuracy: 0.8101 - val_loss: 0.3439\nEpoch 9/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 206ms/step - accuracy: 0.7855 - loss: 0.4883 - val_accuracy: 0.8038 - val_loss: 0.4121\nEpoch 10/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 209ms/step - accuracy: 0.8000 - loss: 0.4632 - val_accuracy: 0.8228 - val_loss: 0.3474\nEpoch 11/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 207ms/step - accuracy: 0.8246 - loss: 0.3764 - val_accuracy: 0.8544 - val_loss: 0.3199\nEpoch 12/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 206ms/step - accuracy: 0.8000 - loss: 0.4234 - val_accuracy: 0.8544 - val_loss: 0.3194\nEpoch 13/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 211ms/step - accuracy: 0.8209 - loss: 0.3905 - val_accuracy: 0.8418 - val_loss: 0.3149\nEpoch 14/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 207ms/step - accuracy: 0.8384 - loss: 0.3405 - val_accuracy: 0.8354 - val_loss: 0.3259\nEpoch 15/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 206ms/step - accuracy: 0.8374 - loss: 0.3710 - val_accuracy: 0.8481 - val_loss: 0.3317\nEpoch 16/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 204ms/step - accuracy: 0.7923 - loss: 0.4167 - val_accuracy: 0.8165 - val_loss: 0.3709\nEpoch 17/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 204ms/step - accuracy: 0.8190 - loss: 0.3765 - val_accuracy: 0.8481 - val_loss: 0.3091\nEpoch 18/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 212ms/step - accuracy: 0.7998 - loss: 0.3918 - val_accuracy: 0.8354 - val_loss: 0.3323\nEpoch 19/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 206ms/step - accuracy: 0.8588 - loss: 0.3233 - val_accuracy: 0.8165 - val_loss: 0.3241\nEpoch 20/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 204ms/step - accuracy: 0.8414 - loss: 0.3350 - val_accuracy: 0.8481 - val_loss: 0.3236\nInitial training time: 191.06 seconds\nEpoch 1/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 599ms/step - accuracy: 0.7215 - loss: 1.5633 - val_accuracy: 0.8165 - val_loss: 0.3338\nEpoch 2/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 215ms/step - accuracy: 0.8294 - loss: 0.4230 - val_accuracy: 0.8987 - val_loss: 0.3470\nEpoch 3/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 213ms/step - accuracy: 0.8267 - loss: 0.4149 - val_accuracy: 0.8418 - val_loss: 0.3662\nEpoch 4/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 215ms/step - accuracy: 0.8374 - loss: 0.3473 - val_accuracy: 0.8418 - val_loss: 0.3978\nEpoch 5/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 214ms/step - accuracy: 0.8579 - loss: 0.3209 - val_accuracy: 0.8165 - val_loss: 0.5584\nEpoch 6/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 211ms/step - accuracy: 0.8926 - loss: 0.2608 - val_accuracy: 0.8608 - val_loss: 0.4112\nEpoch 7/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 211ms/step - accuracy: 0.8698 - loss: 0.2591 - val_accuracy: 0.8797 - val_loss: 0.3426\nEpoch 8/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 214ms/step - accuracy: 0.8999 - loss: 0.2774 - val_accuracy: 0.8481 - val_loss: 0.3662\nEpoch 9/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 207ms/step - accuracy: 0.8913 - loss: 0.2265 - val_accuracy: 0.8608 - val_loss: 0.3419\nEpoch 10/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 212ms/step - accuracy: 0.8801 - loss: 0.2425 - val_accuracy: 0.8671 - val_loss: 0.4171\nEpoch 11/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 225ms/step - accuracy: 0.8836 - loss: 0.2573 - val_accuracy: 0.8734 - val_loss: 0.3933\nEpoch 12/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 219ms/step - accuracy: 0.9007 - loss: 0.2302 - val_accuracy: 0.8797 - val_loss: 0.3363\nEpoch 13/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 216ms/step - accuracy: 0.8973 - loss: 0.2164 - val_accuracy: 0.8924 - val_loss: 0.2813\nEpoch 14/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 218ms/step - accuracy: 0.8951 - loss: 0.2760 - val_accuracy: 0.9114 - val_loss: 0.2683\nEpoch 15/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 215ms/step - accuracy: 0.9147 - loss: 0.1985 - val_accuracy: 0.8987 - val_loss: 0.3078\nEpoch 16/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 211ms/step - accuracy: 0.8958 - loss: 0.1939 - val_accuracy: 0.9114 - val_loss: 0.2792\nEpoch 17/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 214ms/step - accuracy: 0.9035 - loss: 0.2020 - val_accuracy: 0.9051 - val_loss: 0.2304\nEpoch 18/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 217ms/step - accuracy: 0.9126 - loss: 0.1902 - val_accuracy: 0.9114 - val_loss: 0.2045\nEpoch 19/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 217ms/step - accuracy: 0.9218 - loss: 0.1972 - val_accuracy: 0.9051 - val_loss: 0.2797\nEpoch 20/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 212ms/step - accuracy: 0.9236 - loss: 0.1859 - val_accuracy: 0.9177 - val_loss: 0.2334\nFine-tuning training time: 217.88 seconds\n\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 273ms/step - accuracy: 0.8440 - loss: 0.4591\nTest evaluation time: 2.90 seconds\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"# Resnet50+transfer","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2  # OpenCV for general use (not CLAHE)\nimport tensorflow as tf\nimport shutil\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import ResNet50\nfrom sklearn.utils.class_weight import compute_class_weight\nimport time\n\n# Optional: For reproducibility\nimport random\nrandom.seed(42)\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# Define paths\ndataset_path = '/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT'\nbase_dir = '/kaggle/working/split_data'\ntrain_dir = os.path.join(base_dir, 'train')\nval_dir = os.path.join(base_dir, 'validation')\ntest_dir = os.path.join(base_dir, 'test')\n\n# Ensure clean split directories\nfor dir_path in [train_dir, val_dir, test_dir]:\n    if os.path.exists(dir_path):\n        shutil.rmtree(dir_path)\n    os.makedirs(dir_path)\n\n# Split dataset into 70% train, 20% test, 10% validation\nall_images = []\nfor class_name in os.listdir(dataset_path):\n    class_dir = os.path.join(dataset_path, class_name)\n    images = [os.path.join(class_dir, img) for img in os.listdir(class_dir)]\n    all_images.extend([(img, class_name) for img in images])\n\ntest_split = 0.2\nval_split = 0.1\ntrain_images, temp_images = train_test_split(\n    all_images,\n    test_size=(test_split + val_split),\n    stratify=[label for _, label in all_images],\n    random_state=42\n)\nval_images, test_images = train_test_split(\n    temp_images,\n    test_size=(test_split / (test_split + val_split)),\n    stratify=[label for _, label in temp_images],\n    random_state=42\n)\n\n# Copy files into respective directories without preprocessing\ndef copy_images(images, target_dir):\n    for img_path, label in images:\n        label_dir = os.path.join(target_dir, label)\n        os.makedirs(label_dir, exist_ok=True)\n        shutil.copy(img_path, label_dir)  # Copy the original image without preprocessing\n\ncopy_images(train_images, train_dir)\ncopy_images(val_images, val_dir)\ncopy_images(test_images, test_dir)\n\n# Parameters\ninput_shape = (128, 128, 3)\nbatch_size = 32\nepochs = 20\n\n# Create data generators for classification\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\nval_datagen = ImageDataGenerator(rescale=1./255)\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical'  # Categorical classification\n)\n\nvalidation_generator = val_datagen.flow_from_directory(\n    val_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical'  # Categorical classification\n)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',  # Categorical classification\n    shuffle=False\n)\n\n# Print the number of samples in each split\nprint(f\"Training samples: {train_generator.samples}\")\nprint(f\"Validation samples: {validation_generator.samples}\")\nprint(f\"Test samples: {test_generator.samples}\")\n\n# Compute class weights\ntrain_labels = train_generator.classes  # Class indices for training samples\nclass_weights_array = compute_class_weight(\n    class_weight='balanced',  # Balance classes based on their frequency\n    classes=np.unique(train_labels),\n    y=train_labels\n)\nclass_weights = dict(enumerate(class_weights_array))\n\n# Print the calculated class weights\nprint(\"Class Weights:\", class_weights)\n\n# Load pre-trained ResNet50 model + higher-level layers\nbase_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n\n# Freeze the base model\nbase_model.trainable = False\n\n# Add custom layers on top of the base model\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(512, activation='relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(128, activation='relu')(x)\nx = Dropout(0.5)(x)\noutput = Dense(3, activation='softmax')(x)  # Categorical classification\n\n# Create the complete model\nresnet50_model = Model(inputs=base_model.input, outputs=output)\n\n# Compile the model\nresnet50_model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Start profiling for initial training\nstart_time = time.time()\nresnet50_history_initial = resnet50_model.fit(\n    train_generator,\n    epochs=epochs,\n    validation_data=validation_generator,\n    class_weight=class_weights  # Use calculated class weights\n)\nprint(\"Initial training time: {:.2f} seconds\".format(time.time() - start_time))\n\n# Unfreeze the base model for fine-tuning\nbase_model.trainable = True\nresnet50_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])  # Reduce learning rate for fine-tuning\n\n# Start profiling for fine-tuning\nstart_time = time.time()\nresnet50_history_fine = resnet50_model.fit(\n    train_generator,\n    epochs=epochs,\n    validation_data=validation_generator,\n    class_weight=class_weights\n)\nprint(\"Fine-tuning training time: {:.2f} seconds\".format(time.time() - start_time))\n\n# Evaluate the model on the test set\nstart_time = time.time()\nloss, accuracy = resnet50_model.evaluate(test_generator)\nprint(\"Test evaluation time: {:.2f} seconds\".format(time.time() - start_time))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T06:24:25.449833Z","iopub.execute_input":"2025-02-24T06:24:25.450109Z","iopub.status.idle":"2025-02-24T06:32:26.339607Z","shell.execute_reply.started":"2025-02-24T06:24:25.450083Z","shell.execute_reply":"2025-02-24T06:32:26.338899Z"}},"outputs":[{"name":"stdout","text":"Found 1104 images belonging to 3 classes.\nFound 158 images belonging to 3 classes.\nFound 316 images belonging to 3 classes.\nTraining samples: 1104\nValidation samples: 158\nTest samples: 316\nClass Weights: {0: 0.5906902086677368, 1: 1.2474576271186442, 2: 1.978494623655914}\nEpoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 551ms/step - accuracy: 0.3370 - loss: 1.2480 - val_accuracy: 0.3608 - val_loss: 1.0439\nEpoch 2/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 211ms/step - accuracy: 0.3908 - loss: 1.0800 - val_accuracy: 0.3291 - val_loss: 1.0349\nEpoch 3/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 208ms/step - accuracy: 0.3666 - loss: 1.0639 - val_accuracy: 0.4114 - val_loss: 0.9722\nEpoch 4/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 213ms/step - accuracy: 0.3786 - loss: 1.0293 - val_accuracy: 0.5823 - val_loss: 0.9395\nEpoch 5/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 217ms/step - accuracy: 0.3873 - loss: 1.0000 - val_accuracy: 0.3354 - val_loss: 1.0073\nEpoch 6/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 209ms/step - accuracy: 0.3774 - loss: 1.0139 - val_accuracy: 0.4810 - val_loss: 0.9301\nEpoch 7/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 208ms/step - accuracy: 0.4490 - loss: 0.9822 - val_accuracy: 0.3671 - val_loss: 0.9025\nEpoch 8/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 212ms/step - accuracy: 0.4295 - loss: 0.9881 - val_accuracy: 0.5063 - val_loss: 0.9060\nEpoch 9/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 209ms/step - accuracy: 0.4721 - loss: 0.9323 - val_accuracy: 0.5949 - val_loss: 0.8527\nEpoch 10/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 209ms/step - accuracy: 0.5234 - loss: 0.8686 - val_accuracy: 0.5949 - val_loss: 0.8611\nEpoch 11/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 211ms/step - accuracy: 0.5548 - loss: 0.8858 - val_accuracy: 0.5443 - val_loss: 0.8851\nEpoch 12/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 213ms/step - accuracy: 0.5263 - loss: 0.9001 - val_accuracy: 0.7468 - val_loss: 0.7629\nEpoch 13/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 213ms/step - accuracy: 0.5998 - loss: 0.8535 - val_accuracy: 0.5886 - val_loss: 0.8498\nEpoch 14/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 207ms/step - accuracy: 0.5714 - loss: 0.8631 - val_accuracy: 0.6139 - val_loss: 0.8277\nEpoch 15/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 214ms/step - accuracy: 0.5589 - loss: 0.8571 - val_accuracy: 0.6329 - val_loss: 0.8139\nEpoch 16/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 209ms/step - accuracy: 0.5672 - loss: 0.8570 - val_accuracy: 0.6962 - val_loss: 0.7681\nEpoch 17/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 208ms/step - accuracy: 0.6032 - loss: 0.8500 - val_accuracy: 0.5886 - val_loss: 0.8706\nEpoch 18/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 209ms/step - accuracy: 0.5775 - loss: 0.8153 - val_accuracy: 0.6709 - val_loss: 0.7549\nEpoch 19/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 209ms/step - accuracy: 0.5875 - loss: 0.8159 - val_accuracy: 0.7025 - val_loss: 0.7482\nEpoch 20/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 208ms/step - accuracy: 0.6134 - loss: 0.8223 - val_accuracy: 0.6646 - val_loss: 0.7569\nInitial training time: 209.21 seconds\nEpoch 1/20\n\u001b[1m 1/35\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28:14\u001b[0m 50s/step - accuracy: 0.5938 - loss: 22.2304","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1740378529.782014     116 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_26', 16 bytes spill stores, 16 bytes spill loads\n\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 956ms/step - accuracy: 0.6725 - loss: 9.5888 - val_accuracy: 0.2848 - val_loss: 36.3561\nEpoch 2/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 220ms/step - accuracy: 0.6981 - loss: 1.6946 - val_accuracy: 0.3101 - val_loss: 15.1564\nEpoch 3/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 220ms/step - accuracy: 0.7844 - loss: 0.5854 - val_accuracy: 0.2975 - val_loss: 10.0877\nEpoch 4/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 225ms/step - accuracy: 0.7953 - loss: 0.5176 - val_accuracy: 0.2848 - val_loss: 5.0473\nEpoch 5/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 221ms/step - accuracy: 0.8029 - loss: 0.4163 - val_accuracy: 0.2911 - val_loss: 7.4691\nEpoch 6/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 219ms/step - accuracy: 0.8374 - loss: 0.3795 - val_accuracy: 0.2975 - val_loss: 4.1320\nEpoch 7/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 221ms/step - accuracy: 0.8365 - loss: 0.3924 - val_accuracy: 0.4810 - val_loss: 1.0503\nEpoch 8/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 220ms/step - accuracy: 0.8465 - loss: 0.4152 - val_accuracy: 0.5823 - val_loss: 0.9135\nEpoch 9/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 224ms/step - accuracy: 0.8781 - loss: 0.3125 - val_accuracy: 0.5633 - val_loss: 0.8973\nEpoch 10/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 224ms/step - accuracy: 0.8756 - loss: 0.3424 - val_accuracy: 0.5253 - val_loss: 0.9304\nEpoch 11/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 226ms/step - accuracy: 0.8970 - loss: 0.2192 - val_accuracy: 0.4241 - val_loss: 1.1638\nEpoch 12/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 220ms/step - accuracy: 0.8917 - loss: 0.2748 - val_accuracy: 0.5127 - val_loss: 0.9137\nEpoch 13/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 222ms/step - accuracy: 0.9032 - loss: 0.2893 - val_accuracy: 0.5886 - val_loss: 0.8616\nEpoch 14/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 226ms/step - accuracy: 0.9042 - loss: 0.2434 - val_accuracy: 0.5633 - val_loss: 0.8389\nEpoch 15/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 222ms/step - accuracy: 0.9165 - loss: 0.2391 - val_accuracy: 0.5380 - val_loss: 0.7571\nEpoch 16/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 225ms/step - accuracy: 0.8986 - loss: 0.2488 - val_accuracy: 0.6582 - val_loss: 0.6856\nEpoch 17/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 223ms/step - accuracy: 0.9105 - loss: 0.1954 - val_accuracy: 0.6456 - val_loss: 0.6642\nEpoch 18/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 222ms/step - accuracy: 0.8970 - loss: 0.2311 - val_accuracy: 0.7089 - val_loss: 0.5716\nEpoch 19/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 224ms/step - accuracy: 0.9224 - loss: 0.1796 - val_accuracy: 0.7025 - val_loss: 0.5560\nEpoch 20/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 223ms/step - accuracy: 0.9412 - loss: 0.1732 - val_accuracy: 0.6646 - val_loss: 0.6065\nFine-tuning training time: 263.13 seconds\n\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 355ms/step - accuracy: 0.7049 - loss: 0.5661\nTest evaluation time: 3.66 seconds\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"# Vgg19+transfer","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2  # OpenCV for general use (not CLAHE)\nimport tensorflow as tf\nimport shutil\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.applications.vgg19 import preprocess_input\nfrom sklearn.utils.class_weight import compute_class_weight\nimport time\n\n# Optional: For reproducibility\nimport random\nrandom.seed(42)\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# Define paths\ndataset_path = '/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT'\nbase_dir = '/kaggle/working/split_data'\ntrain_dir = os.path.join(base_dir, 'train')\nval_dir = os.path.join(base_dir, 'validation')\ntest_dir = os.path.join(base_dir, 'test')\n\n# Ensure clean split directories\nfor dir_path in [train_dir, val_dir, test_dir]:\n    if os.path.exists(dir_path):\n        shutil.rmtree(dir_path)\n    os.makedirs(dir_path)\n\n# Split dataset into 70% train, 20% test, 10% validation\nall_images = []\nfor class_name in os.listdir(dataset_path):\n    class_dir = os.path.join(dataset_path, class_name)\n    images = [os.path.join(class_dir, img) for img in os.listdir(class_dir)]\n    all_images.extend([(img, class_name) for img in images])\n\ntest_split = 0.2\nval_split = 0.1\ntrain_images, temp_images = train_test_split(\n    all_images,\n    test_size=(test_split + val_split),\n    stratify=[label for _, label in all_images],\n    random_state=42\n)\nval_images, test_images = train_test_split(\n    temp_images,\n    test_size=(test_split / (test_split + val_split)),\n    stratify=[label for _, label in temp_images],\n    random_state=42\n)\n\n# Copy files into respective directories without preprocessing\ndef copy_images(images, target_dir):\n    for img_path, label in images:\n        label_dir = os.path.join(target_dir, label)\n        os.makedirs(label_dir, exist_ok=True)\n        shutil.copy(img_path, label_dir)  # Copy the original image without preprocessing\n\ncopy_images(train_images, train_dir)\ncopy_images(val_images, val_dir)\ncopy_images(test_images, test_dir)\n\n# Parameters\ninput_shape = (128, 128, 3)\nbatch_size = 32\nepochs = 20\n\n# Create data generators for classification\ntrain_datagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input,  # VGG19-specific preprocessing\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\nval_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\ntest_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical'  # Categorical classification\n)\n\nvalidation_generator = val_datagen.flow_from_directory(\n    val_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical'  # Categorical classification\n)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',  # Categorical classification\n    shuffle=False\n)\n\n# Print the number of samples in each split\nprint(f\"Training samples: {train_generator.samples}\")\nprint(f\"Validation samples: {validation_generator.samples}\")\nprint(f\"Test samples: {test_generator.samples}\")\n\n# Compute class weights\ntrain_labels = train_generator.classes  # Class indices for training samples\nclass_weights_array = compute_class_weight(\n    class_weight='balanced',  # Balance classes based on their frequency\n    classes=np.unique(train_labels),\n    y=train_labels\n)\nclass_weights = dict(enumerate(class_weights_array))\n\n# Print the calculated class weights\nprint(\"Class Weights:\", class_weights)\n\n# Load pre-trained VGG19 model + higher-level layers\nbase_model = VGG19(weights='imagenet', include_top=False, input_shape=input_shape)\n\n# Freeze the base model\nbase_model.trainable = False\n\n# Add custom layers on top of the base model\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(512, activation='relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(128, activation='relu')(x)\nx = Dropout(0.5)(x)\noutput = Dense(3, activation='softmax')(x)  # Categorical classification\n\n# Create the complete model\nvgg19_model = Model(inputs=base_model.input, outputs=output)\n\n# Compile the model\nvgg19_model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Start profiling for initial training\nstart_time = time.time()\nvgg19_history_initial = vgg19_model.fit(\n    train_generator,\n    epochs=epochs,\n    validation_data=validation_generator,\n    class_weight=class_weights  # Use calculated class weights\n)\nprint(\"Initial training time: {:.2f} seconds\".format(time.time() - start_time))\n\n# Unfreeze the base model for fine-tuning\nbase_model.trainable = True\nvgg19_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])  # Reduce learning rate for fine-tuning\n\n# Start profiling for fine-tuning\nstart_time = time.time()\nvgg19_history_fine = vgg19_model.fit(\n    train_generator,\n    epochs=epochs,\n    validation_data=validation_generator,\n    class_weight=class_weights\n)\nprint(\"Fine-tuning training time: {:.2f} seconds\".format(time.time() - start_time))\n\n# Evaluate the model on the test set\nstart_time = time.time()\nloss, accuracy = vgg19_model.evaluate(test_generator)\nprint(\"Test evaluation time: {:.2f} seconds\".format(time.time() - start_time))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T06:32:26.341423Z","iopub.execute_input":"2025-02-24T06:32:26.341850Z","iopub.status.idle":"2025-02-24T06:39:23.316551Z","shell.execute_reply.started":"2025-02-24T06:32:26.341806Z","shell.execute_reply":"2025-02-24T06:39:23.315776Z"}},"outputs":[{"name":"stdout","text":"Found 1104 images belonging to 3 classes.\nFound 158 images belonging to 3 classes.\nFound 316 images belonging to 3 classes.\nTraining samples: 1104\nValidation samples: 158\nTest samples: 316\nClass Weights: {0: 0.5906902086677368, 1: 1.2474576271186442, 2: 1.978494623655914}\nEpoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 462ms/step - accuracy: 0.5326 - loss: 3.0884 - val_accuracy: 0.6962 - val_loss: 0.6144\nEpoch 2/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 218ms/step - accuracy: 0.6566 - loss: 1.0949 - val_accuracy: 0.7405 - val_loss: 0.4611\nEpoch 3/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 215ms/step - accuracy: 0.6878 - loss: 0.8228 - val_accuracy: 0.8544 - val_loss: 0.3948\nEpoch 4/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 210ms/step - accuracy: 0.7125 - loss: 0.6798 - val_accuracy: 0.8608 - val_loss: 0.3954\nEpoch 5/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 215ms/step - accuracy: 0.7468 - loss: 0.5527 - val_accuracy: 0.7278 - val_loss: 0.4547\nEpoch 6/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 210ms/step - accuracy: 0.7001 - loss: 0.6938 - val_accuracy: 0.8291 - val_loss: 0.4074\nEpoch 7/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 216ms/step - accuracy: 0.7760 - loss: 0.5131 - val_accuracy: 0.8544 - val_loss: 0.4035\nEpoch 8/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 212ms/step - accuracy: 0.7752 - loss: 0.5089 - val_accuracy: 0.8038 - val_loss: 0.4000\nEpoch 9/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 217ms/step - accuracy: 0.7765 - loss: 0.4949 - val_accuracy: 0.8291 - val_loss: 0.4141\nEpoch 10/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 213ms/step - accuracy: 0.7529 - loss: 0.4918 - val_accuracy: 0.8734 - val_loss: 0.3531\nEpoch 11/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 211ms/step - accuracy: 0.7839 - loss: 0.5229 - val_accuracy: 0.8481 - val_loss: 0.3815\nEpoch 12/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 219ms/step - accuracy: 0.7792 - loss: 0.4630 - val_accuracy: 0.8291 - val_loss: 0.4086\nEpoch 13/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 216ms/step - accuracy: 0.7762 - loss: 0.4476 - val_accuracy: 0.8797 - val_loss: 0.3638\nEpoch 14/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 214ms/step - accuracy: 0.7941 - loss: 0.4687 - val_accuracy: 0.8038 - val_loss: 0.4128\nEpoch 15/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 211ms/step - accuracy: 0.8051 - loss: 0.4283 - val_accuracy: 0.7975 - val_loss: 0.3958\nEpoch 16/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 221ms/step - accuracy: 0.8143 - loss: 0.4037 - val_accuracy: 0.8734 - val_loss: 0.3166\nEpoch 17/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 213ms/step - accuracy: 0.7843 - loss: 0.4526 - val_accuracy: 0.8038 - val_loss: 0.3948\nEpoch 18/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 211ms/step - accuracy: 0.8137 - loss: 0.4146 - val_accuracy: 0.8228 - val_loss: 0.3598\nEpoch 19/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 217ms/step - accuracy: 0.7945 - loss: 0.4591 - val_accuracy: 0.8165 - val_loss: 0.3693\nEpoch 20/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 215ms/step - accuracy: 0.8331 - loss: 0.3819 - val_accuracy: 0.8544 - val_loss: 0.3366\nInitial training time: 197.11 seconds\nEpoch 1/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 446ms/step - accuracy: 0.4454 - loss: 2.0431 - val_accuracy: 0.3481 - val_loss: 0.9854\nEpoch 2/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 231ms/step - accuracy: 0.5349 - loss: 0.9152 - val_accuracy: 0.5316 - val_loss: 1.0039\nEpoch 3/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 228ms/step - accuracy: 0.5058 - loss: 0.9171 - val_accuracy: 0.5696 - val_loss: 0.6101\nEpoch 4/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 235ms/step - accuracy: 0.6532 - loss: 0.6638 - val_accuracy: 0.5949 - val_loss: 0.6002\nEpoch 5/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 227ms/step - accuracy: 0.6444 - loss: 0.5972 - val_accuracy: 0.6646 - val_loss: 0.5709\nEpoch 6/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 229ms/step - accuracy: 0.7257 - loss: 0.5810 - val_accuracy: 0.5886 - val_loss: 0.6015\nEpoch 7/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 228ms/step - accuracy: 0.5640 - loss: 0.7526 - val_accuracy: 0.7089 - val_loss: 0.6536\nEpoch 8/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 229ms/step - accuracy: 0.6647 - loss: 0.6495 - val_accuracy: 0.6392 - val_loss: 0.5417\nEpoch 9/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 227ms/step - accuracy: 0.6155 - loss: 0.7618 - val_accuracy: 0.7785 - val_loss: 0.4630\nEpoch 10/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 225ms/step - accuracy: 0.7406 - loss: 0.5227 - val_accuracy: 0.9051 - val_loss: 0.3427\nEpoch 11/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 234ms/step - accuracy: 0.7975 - loss: 0.4352 - val_accuracy: 0.8481 - val_loss: 0.3767\nEpoch 12/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 228ms/step - accuracy: 0.7832 - loss: 0.5389 - val_accuracy: 0.7848 - val_loss: 0.4084\nEpoch 13/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 228ms/step - accuracy: 0.8214 - loss: 0.4262 - val_accuracy: 0.8291 - val_loss: 0.3716\nEpoch 14/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 234ms/step - accuracy: 0.8293 - loss: 0.3967 - val_accuracy: 0.8734 - val_loss: 0.2753\nEpoch 15/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 227ms/step - accuracy: 0.8413 - loss: 0.3323 - val_accuracy: 0.9304 - val_loss: 0.2472\nEpoch 16/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 231ms/step - accuracy: 0.8630 - loss: 0.3171 - val_accuracy: 0.9051 - val_loss: 0.2381\nEpoch 17/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 235ms/step - accuracy: 0.8780 - loss: 0.2718 - val_accuracy: 0.8861 - val_loss: 0.2665\nEpoch 18/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 226ms/step - accuracy: 0.8588 - loss: 0.2704 - val_accuracy: 0.8608 - val_loss: 0.3124\nEpoch 19/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 231ms/step - accuracy: 0.8486 - loss: 0.3537 - val_accuracy: 0.7215 - val_loss: 0.6986\nEpoch 20/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 230ms/step - accuracy: 0.8202 - loss: 0.3969 - val_accuracy: 0.9177 - val_loss: 0.2342\nFine-tuning training time: 211.81 seconds\n\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 473ms/step - accuracy: 0.8677 - loss: 0.3393\nTest evaluation time: 4.74 seconds\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"# DenseNet121","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport shutil\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import DenseNet121\nfrom sklearn.utils.class_weight import compute_class_weight\nimport time\n\n# Define paths\ndataset_path = '/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT'\nbase_dir = '/kaggle/working/split_data'\ntrain_dir = os.path.join(base_dir, 'train')\nval_dir = os.path.join(base_dir, 'validation')\ntest_dir = os.path.join(base_dir, 'test')\n\n# Ensure clean split directories\nfor dir_path in [train_dir, val_dir, test_dir]:\n    if os.path.exists(dir_path):\n        shutil.rmtree(dir_path)\n    os.makedirs(dir_path)\n\n# Split dataset into 70% train, 20% test, 10% validation\nall_images = []\nfor class_name in os.listdir(dataset_path):\n    class_dir = os.path.join(dataset_path, class_name)\n    images = [os.path.join(class_dir, img) for img in os.listdir(class_dir)]\n    all_images.extend([(img, class_name) for img in images])\n\ntest_split = 0.2\nval_split = 0.1\ntrain_images, temp_images = train_test_split(\n    all_images,\n    test_size=(test_split + val_split),\n    stratify=[label for _, label in all_images],\n    random_state=42\n)\nval_images, test_images = train_test_split(\n    temp_images,\n    test_size=(test_split / (test_split + val_split)),\n    stratify=[label for _, label in temp_images],\n    random_state=42\n)\n\n# Copy files into respective directories without preprocessing\ndef copy_images(images, target_dir):\n    for img_path, label in images:\n        label_dir = os.path.join(target_dir, label)\n        os.makedirs(label_dir, exist_ok=True)\n        shutil.copy(img_path, label_dir)  # Copy the original image without preprocessing\n\ncopy_images(train_images, train_dir)\ncopy_images(val_images, val_dir)\ncopy_images(test_images, test_dir)\n\n# Parameters\ninput_shape = (128, 128, 3)\nbatch_size = 32\nepochs = 20\n\n# Create data generators for classification\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\nval_datagen = ImageDataGenerator(rescale=1./255)\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical'  # Categorical classification\n)\n\nvalidation_generator = val_datagen.flow_from_directory(\n    val_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical'  # Categorical classification\n)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',  # Categorical classification\n    shuffle=False\n)\n\n# Print the number of samples in each split\nprint(f\"Training samples: {train_generator.samples}\")\nprint(f\"Validation samples: {validation_generator.samples}\")\nprint(f\"Test samples: {test_generator.samples}\")\n\n# Compute class weights\ntrain_labels = train_generator.classes  # Class indices for training samples\nclass_weights = compute_class_weight(\n    class_weight='balanced',  # Balance classes based on their frequency\n    classes=np.unique(train_labels),\n    y=train_labels\n)\nclass_weights = dict(enumerate(class_weights))\n\n# Print the calculated class weights\nprint(\"Class Weights:\", class_weights)\n\n# Load pre-trained DenseNet121 model + higher-level layers\nbase_model = DenseNet121(weights='imagenet', include_top=False, input_shape=input_shape)\n\n# Freeze the base model\nbase_model.trainable = False\n\n# Add Global Average Pooling and output layer\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\noutput = Dense(3, activation='softmax')(x)  # Categorical classification output\n\n# Create the complete model\nmodel_densenet = Model(inputs=base_model.input, outputs=output)\n\n# Compile the model\nmodel_densenet.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Start profiling for training\nstart_time = time.time()\nhistory_densenet = model_densenet.fit(\n    train_generator,\n    epochs=epochs,\n    validation_data=validation_generator,\n    class_weight=class_weights\n)\nprint(\"Training time: {:.2f} seconds\".format(time.time() - start_time))\n\n# Evaluate the model on the test set\nstart_time = time.time()\nloss, accuracy = model_densenet.evaluate(test_generator)\nprint(\"Test evaluation time: {:.2f} seconds\".format(time.time() - start_time))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T06:39:23.318784Z","iopub.execute_input":"2025-02-24T06:39:23.319348Z","iopub.status.idle":"2025-02-24T06:43:12.344835Z","shell.execute_reply.started":"2025-02-24T06:39:23.319320Z","shell.execute_reply":"2025-02-24T06:43:12.343973Z"}},"outputs":[{"name":"stdout","text":"Found 1104 images belonging to 3 classes.\nFound 158 images belonging to 3 classes.\nFound 316 images belonging to 3 classes.\nTraining samples: 1104\nValidation samples: 158\nTest samples: 316\nClass Weights: {0: 0.5906902086677368, 1: 1.2474576271186442, 2: 1.978494623655914}\nEpoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 695ms/step - accuracy: 0.2897 - loss: 1.2672 - val_accuracy: 0.5127 - val_loss: 0.9783\nEpoch 2/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 212ms/step - accuracy: 0.4412 - loss: 1.1534 - val_accuracy: 0.6139 - val_loss: 0.8844\nEpoch 3/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 211ms/step - accuracy: 0.5335 - loss: 0.9889 - val_accuracy: 0.6329 - val_loss: 0.8110\nEpoch 4/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 217ms/step - accuracy: 0.5891 - loss: 0.9017 - val_accuracy: 0.6646 - val_loss: 0.7617\nEpoch 5/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 217ms/step - accuracy: 0.6346 - loss: 0.8405 - val_accuracy: 0.6899 - val_loss: 0.7184\nEpoch 6/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 219ms/step - accuracy: 0.6528 - loss: 0.8065 - val_accuracy: 0.6962 - val_loss: 0.6863\nEpoch 7/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 212ms/step - accuracy: 0.6587 - loss: 0.8208 - val_accuracy: 0.7215 - val_loss: 0.6557\nEpoch 8/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 220ms/step - accuracy: 0.6799 - loss: 0.7354 - val_accuracy: 0.7215 - val_loss: 0.6324\nEpoch 9/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 217ms/step - accuracy: 0.7031 - loss: 0.7201 - val_accuracy: 0.6962 - val_loss: 0.6261\nEpoch 10/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 214ms/step - accuracy: 0.7335 - loss: 0.6464 - val_accuracy: 0.7215 - val_loss: 0.6088\nEpoch 11/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 214ms/step - accuracy: 0.6922 - loss: 0.6605 - val_accuracy: 0.7215 - val_loss: 0.5924\nEpoch 12/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 215ms/step - accuracy: 0.7531 - loss: 0.5815 - val_accuracy: 0.7405 - val_loss: 0.5806\nEpoch 13/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 219ms/step - accuracy: 0.6910 - loss: 0.6560 - val_accuracy: 0.7405 - val_loss: 0.5667\nEpoch 14/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 216ms/step - accuracy: 0.7354 - loss: 0.5798 - val_accuracy: 0.7658 - val_loss: 0.5565\nEpoch 15/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 214ms/step - accuracy: 0.7341 - loss: 0.6118 - val_accuracy: 0.7405 - val_loss: 0.5521\nEpoch 16/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 220ms/step - accuracy: 0.7625 - loss: 0.5648 - val_accuracy: 0.7722 - val_loss: 0.5401\nEpoch 17/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 216ms/step - accuracy: 0.7352 - loss: 0.5859 - val_accuracy: 0.7722 - val_loss: 0.5280\nEpoch 18/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 216ms/step - accuracy: 0.7433 - loss: 0.5586 - val_accuracy: 0.7658 - val_loss: 0.5247\nEpoch 19/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 211ms/step - accuracy: 0.7334 - loss: 0.5517 - val_accuracy: 0.7658 - val_loss: 0.5238\nEpoch 20/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 216ms/step - accuracy: 0.7298 - loss: 0.5810 - val_accuracy: 0.7722 - val_loss: 0.5160\nTraining time: 217.84 seconds\n\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 567ms/step - accuracy: 0.7929 - loss: 0.4510\nTest evaluation time: 5.58 seconds\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"# Mobilenet","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport shutil\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import MobileNet\nfrom sklearn.utils.class_weight import compute_class_weight\nimport time\n\n# Define paths\ndataset_path = '/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT'\nbase_dir = '/kaggle/working/split_data'\ntrain_dir = os.path.join(base_dir, 'train')\nval_dir = os.path.join(base_dir, 'validation')\ntest_dir = os.path.join(base_dir, 'test')\n\n# Ensure clean split directories\nfor dir_path in [train_dir, val_dir, test_dir]:\n    if os.path.exists(dir_path):\n        shutil.rmtree(dir_path)\n    os.makedirs(dir_path)\n\n# Split dataset into 70% train, 20% test, 10% validation\nall_images = []\nfor class_name in os.listdir(dataset_path):\n    class_dir = os.path.join(dataset_path, class_name)\n    images = [os.path.join(class_dir, img) for img in os.listdir(class_dir)]\n    all_images.extend([(img, class_name) for img in images])\n\ntest_split = 0.2\nval_split = 0.1\ntrain_images, temp_images = train_test_split(\n    all_images,\n    test_size=(test_split + val_split),\n    stratify=[label for _, label in all_images],\n    random_state=42\n)\nval_images, test_images = train_test_split(\n    temp_images,\n    test_size=(test_split / (test_split + val_split)),\n    stratify=[label for _, label in temp_images],\n    random_state=42\n)\n\n# Copy files into respective directories without preprocessing\ndef copy_images(images, target_dir):\n    for img_path, label in images:\n        label_dir = os.path.join(target_dir, label)\n        os.makedirs(label_dir, exist_ok=True)\n        shutil.copy(img_path, label_dir)  # Copy the original image without preprocessing\n\ncopy_images(train_images, train_dir)\ncopy_images(val_images, val_dir)\ncopy_images(test_images, test_dir)\n\n# Parameters\ninput_shape = (128, 128, 3)\nbatch_size = 32\nepochs = 20\n\n# Create data generators for classification\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\nval_datagen = ImageDataGenerator(rescale=1./255)\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical'  # Categorical classification\n)\n\nvalidation_generator = val_datagen.flow_from_directory(\n    val_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical'  # Categorical classification\n)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',  # Categorical classification\n    shuffle=False\n)\n\n# Print the number of samples in each split\nprint(f\"Training samples: {train_generator.samples}\")\nprint(f\"Validation samples: {validation_generator.samples}\")\nprint(f\"Test samples: {test_generator.samples}\")\n\n# Compute class weights\ntrain_labels = train_generator.classes  # Class indices for training samples\nclass_weights = compute_class_weight(\n    class_weight='balanced',  # Balance classes based on their frequency\n    classes=np.unique(train_labels),\n    y=train_labels\n)\nclass_weights = dict(enumerate(class_weights))\n\n# Print the calculated class weights\nprint(\"Class Weights:\", class_weights)\n\n# Load pre-trained MobileNet model + higher-level layers\nbase_model = MobileNet(weights='imagenet', include_top=False, input_shape=input_shape)\n\n# Freeze the base model\nbase_model.trainable = False\n\n# Add Global Average Pooling and output layer\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\noutput = Dense(3, activation='softmax')(x)  # Categorical classification output\n\n# Create the complete model\nmodel_mobilenet = Model(inputs=base_model.input, outputs=output)\n\n# Compile the model\nmodel_mobilenet.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Start profiling for training\nstart_time = time.time()\nhistory_mobilenet = model_mobilenet.fit(\n    train_generator,\n    epochs=epochs,\n    validation_data=validation_generator,\n    class_weight=class_weights\n)\nprint(\"Training time: {:.2f} seconds\".format(time.time() - start_time))\n\n# Evaluate the model on the test set\nstart_time = time.time()\nloss, accuracy = model_mobilenet.evaluate(test_generator)\nprint(\"Test evaluation time: {:.2f} seconds\".format(time.time() - start_time))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T06:43:12.345931Z","iopub.execute_input":"2025-02-24T06:43:12.346212Z","iopub.status.idle":"2025-02-24T06:46:23.332810Z","shell.execute_reply.started":"2025-02-24T06:43:12.346185Z","shell.execute_reply":"2025-02-24T06:46:23.331969Z"}},"outputs":[{"name":"stdout","text":"Found 1104 images belonging to 3 classes.\nFound 158 images belonging to 3 classes.\nFound 316 images belonging to 3 classes.\nTraining samples: 1104\nValidation samples: 158\nTest samples: 316\nClass Weights: {0: 0.5906902086677368, 1: 1.2474576271186442, 2: 1.978494623655914}\nEpoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 286ms/step - accuracy: 0.5167 - loss: 1.1503 - val_accuracy: 0.4873 - val_loss: 1.0009\nEpoch 2/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 207ms/step - accuracy: 0.5233 - loss: 1.0309 - val_accuracy: 0.5063 - val_loss: 0.9025\nEpoch 3/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 210ms/step - accuracy: 0.5783 - loss: 0.9220 - val_accuracy: 0.5633 - val_loss: 0.8115\nEpoch 4/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 209ms/step - accuracy: 0.6135 - loss: 0.8698 - val_accuracy: 0.6076 - val_loss: 0.7585\nEpoch 5/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 213ms/step - accuracy: 0.6412 - loss: 0.8156 - val_accuracy: 0.6835 - val_loss: 0.7256\nEpoch 6/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 209ms/step - accuracy: 0.7033 - loss: 0.7620 - val_accuracy: 0.7089 - val_loss: 0.6854\nEpoch 7/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 207ms/step - accuracy: 0.7092 - loss: 0.7248 - val_accuracy: 0.7152 - val_loss: 0.6438\nEpoch 8/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 206ms/step - accuracy: 0.7293 - loss: 0.6490 - val_accuracy: 0.7152 - val_loss: 0.6272\nEpoch 9/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 209ms/step - accuracy: 0.7323 - loss: 0.6110 - val_accuracy: 0.7278 - val_loss: 0.6016\nEpoch 10/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 205ms/step - accuracy: 0.7415 - loss: 0.6141 - val_accuracy: 0.7468 - val_loss: 0.5747\nEpoch 11/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 208ms/step - accuracy: 0.7496 - loss: 0.5897 - val_accuracy: 0.7532 - val_loss: 0.5708\nEpoch 12/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 209ms/step - accuracy: 0.7565 - loss: 0.5659 - val_accuracy: 0.7658 - val_loss: 0.5464\nEpoch 13/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 207ms/step - accuracy: 0.7648 - loss: 0.5465 - val_accuracy: 0.7722 - val_loss: 0.5391\nEpoch 14/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 206ms/step - accuracy: 0.7849 - loss: 0.5159 - val_accuracy: 0.7722 - val_loss: 0.5215\nEpoch 15/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 211ms/step - accuracy: 0.7557 - loss: 0.5334 - val_accuracy: 0.7848 - val_loss: 0.5359\nEpoch 16/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 208ms/step - accuracy: 0.7816 - loss: 0.5056 - val_accuracy: 0.7848 - val_loss: 0.4997\nEpoch 17/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 208ms/step - accuracy: 0.7783 - loss: 0.5090 - val_accuracy: 0.7911 - val_loss: 0.5039\nEpoch 18/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 209ms/step - accuracy: 0.7757 - loss: 0.4969 - val_accuracy: 0.7722 - val_loss: 0.5091\nEpoch 19/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 208ms/step - accuracy: 0.7359 - loss: 0.5219 - val_accuracy: 0.7911 - val_loss: 0.4953\nEpoch 20/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 213ms/step - accuracy: 0.7524 - loss: 0.4935 - val_accuracy: 0.7911 - val_loss: 0.4772\nTraining time: 186.21 seconds\n\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 212ms/step - accuracy: 0.7817 - loss: 0.4521\nTest evaluation time: 2.36 seconds\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"# Resnet50","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport shutil\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import ResNet50\nfrom sklearn.utils.class_weight import compute_class_weight\nimport time\n\n# Define paths\ndataset_path = '/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT'\nbase_dir = '/kaggle/working/split_data'\ntrain_dir = os.path.join(base_dir, 'train')\nval_dir = os.path.join(base_dir, 'validation')\ntest_dir = os.path.join(base_dir, 'test')\n\n# Ensure clean split directories\nfor dir_path in [train_dir, val_dir, test_dir]:\n    if os.path.exists(dir_path):\n        shutil.rmtree(dir_path)\n    os.makedirs(dir_path)\n\n# Split dataset into 70% train, 20% test, 10% validation\nall_images = []\nfor class_name in os.listdir(dataset_path):\n    class_dir = os.path.join(dataset_path, class_name)\n    images = [os.path.join(class_dir, img) for img in os.listdir(class_dir)]\n    all_images.extend([(img, class_name) for img in images])\n\ntest_split = 0.2\nval_split = 0.1\ntrain_images, temp_images = train_test_split(\n    all_images,\n    test_size=(test_split + val_split),\n    stratify=[label for _, label in all_images],\n    random_state=42\n)\nval_images, test_images = train_test_split(\n    temp_images,\n    test_size=(test_split / (test_split + val_split)),\n    stratify=[label for _, label in temp_images],\n    random_state=42\n)\n\n# Copy files into respective directories without preprocessing\ndef copy_images(images, target_dir):\n    for img_path, label in images:\n        label_dir = os.path.join(target_dir, label)\n        os.makedirs(label_dir, exist_ok=True)\n        shutil.copy(img_path, label_dir)  # Copy the original image without preprocessing\n\ncopy_images(train_images, train_dir)\ncopy_images(val_images, val_dir)\ncopy_images(test_images, test_dir)\n\n# Parameters\ninput_shape = (128, 128, 3)\nbatch_size = 32\nepochs = 20\n\n# Create data generators for classification\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\nval_datagen = ImageDataGenerator(rescale=1./255)\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical'  # Categorical classification\n)\n\nvalidation_generator = val_datagen.flow_from_directory(\n    val_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical'  # Categorical classification\n)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',  # Categorical classification\n    shuffle=False\n)\n\n# Print the number of samples in each split\nprint(f\"Training samples: {train_generator.samples}\")\nprint(f\"Validation samples: {validation_generator.samples}\")\nprint(f\"Test samples: {test_generator.samples}\")\n\n# Compute class weights\ntrain_labels = train_generator.classes  # Class indices for training samples\nclass_weights = compute_class_weight(\n    class_weight='balanced',  # Balance classes based on their frequency\n    classes=np.unique(train_labels),\n    y=train_labels\n)\nclass_weights = dict(enumerate(class_weights))\n\n# Print the calculated class weights\nprint(\"Class Weights:\", class_weights)\n\n# Load pre-trained ResNet50 model + higher-level layers\nbase_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n\n# Freeze the base model\nbase_model.trainable = False\n\n# Add Global Average Pooling and output layer\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\noutput = Dense(3, activation='softmax')(x)  # Categorical classification output\n\n# Create the complete model\nmodel_resnet = Model(inputs=base_model.input, outputs=output)\n\n# Compile the model\nmodel_resnet.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Start profiling for training\nstart_time = time.time()\nhistory_resnet = model_resnet.fit(\n    train_generator,\n    epochs=epochs,\n    validation_data=validation_generator,\n    class_weight=class_weights\n)\nprint(\"Training time: {:.2f} seconds\".format(time.time() - start_time))\n\n# Evaluate the model on the test set\nstart_time = time.time()\nloss, accuracy = model_resnet.evaluate(test_generator)\nprint(\"Test evaluation time: {:.2f} seconds\".format(time.time() - start_time))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T06:46:23.333835Z","iopub.execute_input":"2025-02-24T06:46:23.334111Z","iopub.status.idle":"2025-02-24T06:49:44.739406Z","shell.execute_reply.started":"2025-02-24T06:46:23.334086Z","shell.execute_reply":"2025-02-24T06:49:44.738523Z"}},"outputs":[{"name":"stdout","text":"Found 1104 images belonging to 3 classes.\nFound 158 images belonging to 3 classes.\nFound 316 images belonging to 3 classes.\nTraining samples: 1104\nValidation samples: 158\nTest samples: 316\nClass Weights: {0: 0.5906902086677368, 1: 1.2474576271186442, 2: 1.978494623655914}\nEpoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 382ms/step - accuracy: 0.2697 - loss: 1.4631 - val_accuracy: 0.2658 - val_loss: 1.6134\nEpoch 2/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 217ms/step - accuracy: 0.2747 - loss: 1.3067 - val_accuracy: 0.2658 - val_loss: 1.4608\nEpoch 3/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 214ms/step - accuracy: 0.2646 - loss: 1.2299 - val_accuracy: 0.2658 - val_loss: 1.3431\nEpoch 4/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 212ms/step - accuracy: 0.2672 - loss: 1.1667 - val_accuracy: 0.2658 - val_loss: 1.2544\nEpoch 5/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 211ms/step - accuracy: 0.2608 - loss: 1.1323 - val_accuracy: 0.2595 - val_loss: 1.1870\nEpoch 6/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 210ms/step - accuracy: 0.2635 - loss: 1.1075 - val_accuracy: 0.2658 - val_loss: 1.1426\nEpoch 7/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 216ms/step - accuracy: 0.2741 - loss: 1.0955 - val_accuracy: 0.3228 - val_loss: 1.1133\nEpoch 8/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 209ms/step - accuracy: 0.2520 - loss: 1.0874 - val_accuracy: 0.2342 - val_loss: 1.0920\nEpoch 9/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 208ms/step - accuracy: 0.2554 - loss: 1.0802 - val_accuracy: 0.2405 - val_loss: 1.0763\nEpoch 10/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 208ms/step - accuracy: 0.2331 - loss: 1.0541 - val_accuracy: 0.3291 - val_loss: 1.0651\nEpoch 11/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 213ms/step - accuracy: 0.2973 - loss: 1.0461 - val_accuracy: 0.3418 - val_loss: 1.0570\nEpoch 12/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 209ms/step - accuracy: 0.3158 - loss: 1.0692 - val_accuracy: 0.3544 - val_loss: 1.0545\nEpoch 13/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 209ms/step - accuracy: 0.3441 - loss: 1.0569 - val_accuracy: 0.3734 - val_loss: 1.0504\nEpoch 14/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 212ms/step - accuracy: 0.3406 - loss: 1.0705 - val_accuracy: 0.3734 - val_loss: 1.0474\nEpoch 15/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 212ms/step - accuracy: 0.3673 - loss: 1.0701 - val_accuracy: 0.4114 - val_loss: 1.0398\nEpoch 16/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 208ms/step - accuracy: 0.3350 - loss: 1.0342 - val_accuracy: 0.4051 - val_loss: 1.0367\nEpoch 17/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 214ms/step - accuracy: 0.3582 - loss: 1.0359 - val_accuracy: 0.4114 - val_loss: 1.0346\nEpoch 18/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 217ms/step - accuracy: 0.3665 - loss: 1.0539 - val_accuracy: 0.3924 - val_loss: 1.0343\nEpoch 19/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 209ms/step - accuracy: 0.3498 - loss: 1.0504 - val_accuracy: 0.4177 - val_loss: 1.0301\nEpoch 20/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 210ms/step - accuracy: 0.3743 - loss: 1.0563 - val_accuracy: 0.3924 - val_loss: 1.0281\nTraining time: 195.19 seconds\n\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 264ms/step - accuracy: 0.2312 - loss: 1.0511\nTest evaluation time: 2.85 seconds\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"# Vgg19","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport shutil\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import VGG19\nfrom sklearn.utils.class_weight import compute_class_weight\nimport time\n\n# Define paths\ndataset_path = '/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT'\nbase_dir = '/kaggle/working/split_data'\ntrain_dir = os.path.join(base_dir, 'train')\nval_dir = os.path.join(base_dir, 'validation')\ntest_dir = os.path.join(base_dir, 'test')\n\n# Ensure clean split directories\nfor dir_path in [train_dir, val_dir, test_dir]:\n    if os.path.exists(dir_path):\n        shutil.rmtree(dir_path)\n    os.makedirs(dir_path)\n\n# Split dataset into 70% train, 20% test, 10% validation\nall_images = []\nfor class_name in os.listdir(dataset_path):\n    class_dir = os.path.join(dataset_path, class_name)\n    images = [os.path.join(class_dir, img) for img in os.listdir(class_dir)]\n    all_images.extend([(img, class_name) for img in images])\n\ntest_split = 0.2\nval_split = 0.1\ntrain_images, temp_images = train_test_split(\n    all_images,\n    test_size=(test_split + val_split),\n    stratify=[label for _, label in all_images],\n    random_state=42\n)\nval_images, test_images = train_test_split(\n    temp_images,\n    test_size=(test_split / (test_split + val_split)),\n    stratify=[label for _, label in temp_images],\n    random_state=42\n)\n\n# Copy files into respective directories without preprocessing\ndef copy_images(images, target_dir):\n    for img_path, label in images:\n        label_dir = os.path.join(target_dir, label)\n        os.makedirs(label_dir, exist_ok=True)\n        shutil.copy(img_path, label_dir)  # Copy the original image without preprocessing\n\ncopy_images(train_images, train_dir)\ncopy_images(val_images, val_dir)\ncopy_images(test_images, test_dir)\n\n# Parameters\ninput_shape = (128, 128, 3)\nbatch_size = 32\nepochs = 20\n\n# Create data generators for classification\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\nval_datagen = ImageDataGenerator(rescale=1./255)\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical'  # Categorical classification\n)\n\nvalidation_generator = val_datagen.flow_from_directory(\n    val_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical'  # Categorical classification\n)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',  # Categorical classification\n    shuffle=False\n)\n\n# Print the number of samples in each split\nprint(f\"Training samples: {train_generator.samples}\")\nprint(f\"Validation samples: {validation_generator.samples}\")\nprint(f\"Test samples: {test_generator.samples}\")\n\n# Compute class weights\ntrain_labels = train_generator.classes  # Class indices for training samples\nclass_weights = compute_class_weight(\n    class_weight='balanced',  # Balance classes based on their frequency\n    classes=np.unique(train_labels),\n    y=train_labels\n)\nclass_weights = dict(enumerate(class_weights))\n\n# Print the calculated class weights\nprint(\"Class Weights:\", class_weights)\n\n# Load pre-trained VGG19 model + higher-level layers\nbase_model = VGG19(weights='imagenet', include_top=False, input_shape=input_shape)\n\n# Freeze the base model\nbase_model.trainable = False\n\n# Add Global Average Pooling and output layer\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\noutput = Dense(3, activation='softmax')(x)  # Categorical classification output\n\n# Create the complete model\nmodel_vgg = Model(inputs=base_model.input, outputs=output)\n\n# Compile the model\nmodel_vgg.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Start profiling for training\nstart_time = time.time()\nhistory_vgg = model_vgg.fit(\n    train_generator,\n    epochs=epochs,\n    validation_data=validation_generator,\n    class_weight=class_weights\n)\nprint(\"Training time: {:.2f} seconds\".format(time.time() - start_time))\n\n# Evaluate the model on the test set\nstart_time = time.time()\nloss, accuracy = model_vgg.evaluate(test_generator)\nprint(\"Test evaluation time: {:.2f} seconds\".format(time.time() - start_time))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T06:49:44.740628Z","iopub.execute_input":"2025-02-24T06:49:44.740927Z","iopub.status.idle":"2025-02-24T06:53:00.763092Z","shell.execute_reply.started":"2025-02-24T06:49:44.740901Z","shell.execute_reply":"2025-02-24T06:53:00.762238Z"}},"outputs":[{"name":"stdout","text":"Found 1104 images belonging to 3 classes.\nFound 158 images belonging to 3 classes.\nFound 316 images belonging to 3 classes.\nTraining samples: 1104\nValidation samples: 158\nTest samples: 316\nClass Weights: {0: 0.5906902086677368, 1: 1.2474576271186442, 2: 1.978494623655914}\nEpoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 256ms/step - accuracy: 0.5736 - loss: 1.2961 - val_accuracy: 0.5633 - val_loss: 1.0309\nEpoch 2/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 214ms/step - accuracy: 0.5762 - loss: 1.2009 - val_accuracy: 0.5127 - val_loss: 1.0420\nEpoch 3/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 213ms/step - accuracy: 0.5240 - loss: 1.1598 - val_accuracy: 0.4620 - val_loss: 1.0587\nEpoch 4/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 213ms/step - accuracy: 0.4850 - loss: 1.1267 - val_accuracy: 0.3924 - val_loss: 1.0720\nEpoch 5/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 211ms/step - accuracy: 0.5003 - loss: 1.0997 - val_accuracy: 0.3861 - val_loss: 1.0777\nEpoch 6/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 215ms/step - accuracy: 0.4484 - loss: 1.1275 - val_accuracy: 0.3861 - val_loss: 1.0832\nEpoch 7/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 228ms/step - accuracy: 0.4542 - loss: 1.1236 - val_accuracy: 0.4367 - val_loss: 1.0762\nEpoch 8/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 220ms/step - accuracy: 0.4634 - loss: 1.0935 - val_accuracy: 0.4430 - val_loss: 1.0649\nEpoch 9/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 213ms/step - accuracy: 0.4890 - loss: 1.0698 - val_accuracy: 0.4494 - val_loss: 1.0608\nEpoch 10/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 219ms/step - accuracy: 0.4280 - loss: 1.1036 - val_accuracy: 0.4494 - val_loss: 1.0499\nEpoch 11/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 223ms/step - accuracy: 0.5086 - loss: 1.0332 - val_accuracy: 0.4557 - val_loss: 1.0396\nEpoch 12/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 222ms/step - accuracy: 0.4895 - loss: 1.0480 - val_accuracy: 0.4684 - val_loss: 1.0285\nEpoch 13/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 220ms/step - accuracy: 0.5085 - loss: 1.0280 - val_accuracy: 0.4747 - val_loss: 1.0174\nEpoch 14/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 230ms/step - accuracy: 0.5028 - loss: 1.0023 - val_accuracy: 0.4873 - val_loss: 1.0105\nEpoch 15/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 225ms/step - accuracy: 0.5119 - loss: 1.0051 - val_accuracy: 0.5696 - val_loss: 1.0014\nEpoch 16/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 223ms/step - accuracy: 0.5681 - loss: 1.0027 - val_accuracy: 0.5633 - val_loss: 0.9957\nEpoch 17/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 228ms/step - accuracy: 0.5812 - loss: 0.9808 - val_accuracy: 0.5759 - val_loss: 0.9886\nEpoch 18/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 228ms/step - accuracy: 0.5723 - loss: 0.9882 - val_accuracy: 0.5633 - val_loss: 0.9780\nEpoch 19/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 219ms/step - accuracy: 0.6131 - loss: 0.9865 - val_accuracy: 0.5696 - val_loss: 0.9713\nEpoch 20/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 221ms/step - accuracy: 0.5851 - loss: 0.9893 - val_accuracy: 0.5759 - val_loss: 0.9565\nTraining time: 191.46 seconds\n\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 176ms/step - accuracy: 0.5738 - loss: 0.9503\nTest evaluation time: 2.09 seconds\n","output_type":"stream"}],"execution_count":25}]}