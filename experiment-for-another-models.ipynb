{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2021025,"sourceType":"datasetVersion","datasetId":1209633},{"sourceId":9073122,"sourceType":"datasetVersion","datasetId":5472989}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Experiment","metadata":{}},{"cell_type":"code","source":"print(\"Start\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T03:17:14.621746Z","iopub.execute_input":"2024-11-30T03:17:14.621990Z","iopub.status.idle":"2024-11-30T03:17:14.633671Z","shell.execute_reply.started":"2024-11-30T03:17:14.621966Z","shell.execute_reply":"2024-11-30T03:17:14.632935Z"}},"outputs":[{"name":"stdout","text":"Start\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# AlexNet","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport shutil\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# Define paths\ndataset_path = '/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT'\nbase_dir = '/kaggle/working/split_data'\ntrain_dir = os.path.join(base_dir, 'train')\nval_dir = os.path.join(base_dir, 'validation')\ntest_dir = os.path.join(base_dir, 'test')\n\n# Ensure clean split directories\nfor dir_path in [train_dir, val_dir, test_dir]:\n    if os.path.exists(dir_path):\n        shutil.rmtree(dir_path)\n    os.makedirs(dir_path)\n\n# Split dataset into 70% train, 20% test, 10% validation\nall_images = []\nfor class_name in os.listdir(dataset_path):\n    class_dir = os.path.join(dataset_path, class_name)\n    images = [os.path.join(class_dir, img) for img in os.listdir(class_dir)]\n    all_images.extend([(img, class_name) for img in images])\n\ntest_split = 0.2\nval_split = 0.1\ntrain_images, temp_images = train_test_split(\n    all_images,\n    test_size=(test_split + val_split),\n    stratify=[label for _, label in all_images],\n    random_state=42\n)\nval_images, test_images = train_test_split(\n    temp_images,\n    test_size=(test_split / (test_split + val_split)),\n    stratify=[label for _, label in temp_images],\n    random_state=42\n)\n\n# Function to copy images to respective directories\ndef copy_images(images, target_dir):\n    for img_path, label in images:\n        label_dir = os.path.join(target_dir, label)\n        os.makedirs(label_dir, exist_ok=True)\n        shutil.copy(img_path, label_dir)\n\ncopy_images(train_images, train_dir)\ncopy_images(val_images, val_dir)\ncopy_images(test_images, test_dir)\n\n# Parameters\ninput_shape = (227, 227, 3)  # Input shape for AlexNet\nbatch_size = 32\nepochs = 20\n\n# Create data generators\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\nval_datagen = ImageDataGenerator(rescale=1./255)\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical'\n)\n\nvalidation_generator = val_datagen.flow_from_directory(\n    val_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical'\n)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    shuffle=False\n)\n\n# Print the number of samples in each split\nprint(f\"Training samples: {train_generator.samples}\")\nprint(f\"Validation samples: {validation_generator.samples}\")\nprint(f\"Test samples: {test_generator.samples}\")\n\n# Compute class weights\ntrain_labels = train_generator.classes\nclass_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(train_labels),\n    y=train_labels\n)\nclass_weights = dict(enumerate(class_weights))\n\n# Print the calculated class weights\nprint(\"Class Weights:\", class_weights)\n\n# Build the AlexNet model\nmodel_alexnet = Sequential()\n\n# Layer 1: Convolutional Layer\nmodel_alexnet.add(Conv2D(96, (11, 11), strides=(4, 4), activation='relu', input_shape=input_shape))\nmodel_alexnet.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n\n# Layer 2: Convolutional Layer\nmodel_alexnet.add(Conv2D(256, (5, 5), padding='same', activation='relu'))\nmodel_alexnet.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n\n# Layer 3: Convolutional Layer\nmodel_alexnet.add(Conv2D(384, (3, 3), padding='same', activation='relu'))\n\n# Layer 4: Convolutional Layer\nmodel_alexnet.add(Conv2D(384, (3, 3), padding='same', activation='relu'))\n\n# Layer 5: Convolutional Layer\nmodel_alexnet.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\nmodel_alexnet.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n\n# Flatten the output\nmodel_alexnet.add(Flatten())\n\n# Fully Connected Layers\nmodel_alexnet.add(Dense(4096, activation='relu'))\n\nmodel_alexnet.add(Dense(4096, activation='relu'))\n\nmodel_alexnet.add(Dense(3, activation='softmax'))  # Adjust for the number of classes\n\n# Compile the model\nmodel_alexnet.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Fine-tune the model with class weights\nhistory_alexnet = model_alexnet.fit(\n    train_generator,\n    epochs=epochs,\n    validation_data=validation_generator,\n    class_weight=class_weights\n)\n\n# Evaluate the model on the test set\nloss, accuracy = model_alexnet.evaluate(test_generator)\nprint('Loss:', loss)\nprint(f'Test Accuracy: {accuracy * 100:.2f}%')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T03:17:36.450715Z","iopub.execute_input":"2024-11-30T03:17:36.451062Z","iopub.status.idle":"2024-11-30T03:23:03.858689Z","shell.execute_reply.started":"2024-11-30T03:17:36.451032Z","shell.execute_reply":"2024-11-30T03:23:03.857831Z"}},"outputs":[{"name":"stdout","text":"Found 1104 images belonging to 3 classes.\nFound 158 images belonging to 3 classes.\nFound 316 images belonging to 3 classes.\nTraining samples: 1104\nValidation samples: 158\nTest samples: 316\nClass Weights: {0: 0.5906902086677368, 1: 1.2474576271186442, 2: 1.978494623655914}\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 394ms/step - accuracy: 0.4393 - loss: 1.0485 - val_accuracy: 0.4494 - val_loss: 0.8981\nEpoch 2/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 360ms/step - accuracy: 0.4991 - loss: 0.9205 - val_accuracy: 0.4051 - val_loss: 0.9306\nEpoch 3/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 363ms/step - accuracy: 0.4763 - loss: 0.8022 - val_accuracy: 0.4810 - val_loss: 0.7535\nEpoch 4/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 364ms/step - accuracy: 0.5052 - loss: 0.7649 - val_accuracy: 0.5759 - val_loss: 0.7625\nEpoch 5/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 365ms/step - accuracy: 0.5327 - loss: 0.7368 - val_accuracy: 0.5380 - val_loss: 0.7813\nEpoch 6/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 363ms/step - accuracy: 0.5874 - loss: 0.6928 - val_accuracy: 0.5506 - val_loss: 0.7337\nEpoch 7/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 366ms/step - accuracy: 0.6323 - loss: 0.6590 - val_accuracy: 0.7089 - val_loss: 0.6842\nEpoch 8/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 372ms/step - accuracy: 0.5816 - loss: 0.6889 - val_accuracy: 0.6392 - val_loss: 0.6669\nEpoch 9/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 368ms/step - accuracy: 0.6251 - loss: 0.6528 - val_accuracy: 0.6646 - val_loss: 0.6928\nEpoch 10/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 361ms/step - accuracy: 0.6555 - loss: 0.6596 - val_accuracy: 0.6203 - val_loss: 0.6827\nEpoch 11/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 375ms/step - accuracy: 0.6685 - loss: 0.5974 - val_accuracy: 0.6835 - val_loss: 0.5616\nEpoch 12/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 362ms/step - accuracy: 0.6890 - loss: 0.5743 - val_accuracy: 0.8101 - val_loss: 0.4882\nEpoch 13/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 363ms/step - accuracy: 0.7416 - loss: 0.5317 - val_accuracy: 0.7848 - val_loss: 0.4922\nEpoch 14/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 360ms/step - accuracy: 0.7254 - loss: 0.5445 - val_accuracy: 0.7658 - val_loss: 0.5003\nEpoch 15/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 365ms/step - accuracy: 0.7365 - loss: 0.5137 - val_accuracy: 0.8101 - val_loss: 0.4089\nEpoch 16/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 364ms/step - accuracy: 0.7601 - loss: 0.5013 - val_accuracy: 0.7848 - val_loss: 0.4416\nEpoch 17/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 357ms/step - accuracy: 0.7823 - loss: 0.4798 - val_accuracy: 0.7848 - val_loss: 0.5295\nEpoch 18/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 360ms/step - accuracy: 0.8061 - loss: 0.4702 - val_accuracy: 0.7722 - val_loss: 0.4907\nEpoch 19/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 358ms/step - accuracy: 0.8112 - loss: 0.4011 - val_accuracy: 0.8354 - val_loss: 0.4060\nEpoch 20/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 364ms/step - accuracy: 0.8013 - loss: 0.4308 - val_accuracy: 0.6139 - val_loss: 0.7690\n\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 167ms/step - accuracy: 0.4789 - loss: 1.0624\nLoss: 0.8698810338973999\nTest Accuracy: 58.86%\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# EfficientNetB0","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport shutil\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import EfficientNetB0  # Import EfficientNet\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# Define paths\ndataset_path = '/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT'\nbase_dir = '/kaggle/working/split_data'\ntrain_dir = os.path.join(base_dir, 'train')\nval_dir = os.path.join(base_dir, 'validation')\ntest_dir = os.path.join(base_dir, 'test')\n\n# Ensure clean split directories\nfor dir_path in [train_dir, val_dir, test_dir]:\n    if os.path.exists(dir_path):\n        shutil.rmtree(dir_path)\n    os.makedirs(dir_path)\n\n# Split dataset into 70% train, 20% test, 10% validation\nall_images = []\nfor class_name in os.listdir(dataset_path):\n    class_dir = os.path.join(dataset_path, class_name)\n    images = [os.path.join(class_dir, img) for img in os.listdir(class_dir)]\n    all_images.extend([(img, class_name) for img in images])\n\ntest_split = 0.2\nval_split = 0.1\ntrain_images, temp_images = train_test_split(\n    all_images,\n    test_size=(test_split + val_split),\n    stratify=[label for _, label in all_images],\n    random_state=42\n)\nval_images, test_images = train_test_split(\n    temp_images,\n    test_size=(test_split / (test_split + val_split)),\n    stratify=[label for _, label in temp_images],\n    random_state=42\n)\n\n# Function to copy images to respective directories\ndef copy_images(images, target_dir):\n    for img_path, label in images:\n        label_dir = os.path.join(target_dir, label)\n        os.makedirs(label_dir, exist_ok=True)\n        shutil.copy(img_path, label_dir)\n\ncopy_images(train_images, train_dir)\ncopy_images(val_images, val_dir)\ncopy_images(test_images, test_dir)\n\n# Parameters\ninput_shape = (128, 128, 3)  # EfficientNet typically needs larger input sizes, adjust if needed\nbatch_size = 32\nepochs = 20\n\n# Create data generators\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\nval_datagen = ImageDataGenerator(rescale=1./255)\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical'\n)\n\nvalidation_generator = val_datagen.flow_from_directory(\n    val_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical'\n)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    shuffle=False\n)\n\n# Print the number of samples in each split\nprint(f\"Training samples: {train_generator.samples}\")\nprint(f\"Validation samples: {validation_generator.samples}\")\nprint(f\"Test samples: {test_generator.samples}\")\n\n# Compute class weights\ntrain_labels = train_generator.classes\nclass_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(train_labels),\n    y=train_labels\n)\nclass_weights = dict(enumerate(class_weights))\n\n# Print the calculated class weights\nprint(\"Class Weights:\", class_weights)\n\n# Load pre-trained EfficientNet model + higher-level layers\nbase_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)  # Adjust accordingly\n\n# Freeze the base model\nbase_model.trainable = False\n\n# Add Global Average Pooling and output layer\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\noutput = Dense(3, activation='softmax')(x)  # Adjust the number of classes if needed\n\n# Create the complete model\nmodel_efficientnet = Model(inputs=base_model.input, outputs=output)\n\n# Compile the model\nmodel_efficientnet.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Fine-tune the model with class weights\nhistory_efficientnet = model_efficientnet.fit(\n    train_generator,\n    epochs=epochs,\n    validation_data=validation_generator,\n    class_weight=class_weights\n)\n\n# Evaluate the model on the test set\nloss, accuracy = model_efficientnet.evaluate(test_generator)\nprint('Loss:', loss)\nprint(f'Test Accuracy: {accuracy * 100:.2f}%')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T06:38:05.935530Z","iopub.execute_input":"2024-11-29T06:38:05.936054Z","iopub.status.idle":"2024-11-29T06:41:57.066451Z","shell.execute_reply.started":"2024-11-29T06:38:05.936011Z","shell.execute_reply":"2024-11-29T06:41:57.065593Z"}},"outputs":[{"name":"stdout","text":"Found 1104 images belonging to 3 classes.\nFound 158 images belonging to 3 classes.\nFound 316 images belonging to 3 classes.\nTraining samples: 1104\nValidation samples: 158\nTest samples: 316\nClass Weights: {0: 0.5906902086677368, 1: 1.2474576271186442, 2: 1.978494623655914}\nDownloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n\u001b[1m16705208/16705208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nEpoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 840ms/step - accuracy: 0.1752 - loss: 1.0919 - val_accuracy: 0.3418 - val_loss: 1.1070\nEpoch 2/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 205ms/step - accuracy: 0.3386 - loss: 1.0773 - val_accuracy: 0.2975 - val_loss: 1.1022\nEpoch 3/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 205ms/step - accuracy: 0.2945 - loss: 1.1189 - val_accuracy: 0.2658 - val_loss: 1.1023\nEpoch 4/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 201ms/step - accuracy: 0.3037 - loss: 1.1130 - val_accuracy: 0.5633 - val_loss: 1.0958\nEpoch 5/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 201ms/step - accuracy: 0.3224 - loss: 1.1210 - val_accuracy: 0.1772 - val_loss: 1.1002\nEpoch 6/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 208ms/step - accuracy: 0.2930 - loss: 1.1013 - val_accuracy: 0.2658 - val_loss: 1.0949\nEpoch 7/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 203ms/step - accuracy: 0.3317 - loss: 1.1092 - val_accuracy: 0.2658 - val_loss: 1.0982\nEpoch 8/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 207ms/step - accuracy: 0.3936 - loss: 1.0728 - val_accuracy: 0.3797 - val_loss: 1.0972\nEpoch 9/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 205ms/step - accuracy: 0.3112 - loss: 1.1005 - val_accuracy: 0.1772 - val_loss: 1.1003\nEpoch 10/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 208ms/step - accuracy: 0.3140 - loss: 1.0804 - val_accuracy: 0.2658 - val_loss: 1.1110\nEpoch 11/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 202ms/step - accuracy: 0.2599 - loss: 1.1087 - val_accuracy: 0.5633 - val_loss: 1.0952\nEpoch 12/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 201ms/step - accuracy: 0.4028 - loss: 1.0909 - val_accuracy: 0.2658 - val_loss: 1.0929\nEpoch 13/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 206ms/step - accuracy: 0.2940 - loss: 1.1042 - val_accuracy: 0.1772 - val_loss: 1.1001\nEpoch 14/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 204ms/step - accuracy: 0.3399 - loss: 1.0974 - val_accuracy: 0.2848 - val_loss: 1.0938\nEpoch 15/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 208ms/step - accuracy: 0.3875 - loss: 1.0901 - val_accuracy: 0.2785 - val_loss: 1.1019\nEpoch 16/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 201ms/step - accuracy: 0.2597 - loss: 1.1262 - val_accuracy: 0.1709 - val_loss: 1.1013\nEpoch 17/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 203ms/step - accuracy: 0.4486 - loss: 1.0954 - val_accuracy: 0.2658 - val_loss: 1.0987\nEpoch 18/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 219ms/step - accuracy: 0.4288 - loss: 1.0982 - val_accuracy: 0.1709 - val_loss: 1.1037\nEpoch 19/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 211ms/step - accuracy: 0.2832 - loss: 1.1123 - val_accuracy: 0.2658 - val_loss: 1.1029\nEpoch 20/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 203ms/step - accuracy: 0.3156 - loss: 1.1065 - val_accuracy: 0.2658 - val_loss: 1.0989\n\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 529ms/step - accuracy: 0.1266 - loss: 1.1004\nLoss: 1.098688006401062\nTest Accuracy: 26.58%\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# CNN Layers","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport shutil\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# Define paths\ndataset_path = '/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT'\nbase_dir = '/kaggle/working/split_data'\ntrain_dir = os.path.join(base_dir, 'train')\nval_dir = os.path.join(base_dir, 'validation')\ntest_dir = os.path.join(base_dir, 'test')\n\n# Ensure clean split directories\nfor dir_path in [train_dir, val_dir, test_dir]:\n    if os.path.exists(dir_path):\n        shutil.rmtree(dir_path)\n    os.makedirs(dir_path)\n\n# Split dataset into 70% train, 20% test, 10% validation\nall_images = []\nfor class_name in os.listdir(dataset_path):\n    class_dir = os.path.join(dataset_path, class_name)\n    images = [os.path.join(class_dir, img) for img in os.listdir(class_dir)]\n    all_images.extend([(img, class_name) for img in images])\n\ntest_split = 0.2\nval_split = 0.1\ntrain_images, temp_images = train_test_split(\n    all_images,\n    test_size=(test_split + val_split),\n    stratify=[label for _, label in all_images],\n    random_state=42\n)\nval_images, test_images = train_test_split(\n    temp_images,\n    test_size=(test_split / (test_split + val_split)),\n    stratify=[label for _, label in temp_images],\n    random_state=42\n)\n\n# Function to copy images to respective directories\ndef copy_images(images, target_dir):\n    for img_path, label in images:\n        label_dir = os.path.join(target_dir, label)\n        os.makedirs(label_dir, exist_ok=True)\n        shutil.copy(img_path, label_dir)\n\ncopy_images(train_images, train_dir)\ncopy_images(val_images, val_dir)\ncopy_images(test_images, test_dir)\n\n# Parameters\ninput_shape = (128, 128, 3)  # Input shape for the images\nbatch_size = 32\nepochs = 20\n\n# Create data generators\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\nval_datagen = ImageDataGenerator(rescale=1./255)\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical'\n)\n\nvalidation_generator = val_datagen.flow_from_directory(\n    val_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical'\n)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=input_shape[:2],\n    batch_size=batch_size,\n    class_mode='categorical',\n    shuffle=False\n)\n\n# Print the number of samples in each split\nprint(f\"Training samples: {train_generator.samples}\")\nprint(f\"Validation samples: {validation_generator.samples}\")\nprint(f\"Test samples: {test_generator.samples}\")\n\n# Compute class weights\ntrain_labels = train_generator.classes\nclass_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(train_labels),\n    y=train_labels\n)\nclass_weights = dict(enumerate(class_weights))\n\n# Print the calculated class weights\nprint(\"Class Weights:\", class_weights)\n\n# Build a simple CNN model\nmodel_cnn = Sequential()\n\n# Convolutional layers\nmodel_cnn.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\nmodel_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_cnn.add(Conv2D(64, (3, 3), activation='relu'))\nmodel_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel_cnn.add(Conv2D(128, (3, 3), activation='relu'))\nmodel_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n\n# Flattening\nmodel_cnn.add(Flatten())\n\n# Fully connected layers\nmodel_cnn.add(Dense(128, activation='relu'))\nmodel_cnn.add(Dropout(0.5))  # Dropout for regularization\nmodel_cnn.add(Dense(3, activation='softmax'))  # Adjust for the number of classes\n\n# Compile the model\nmodel_cnn.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Fine-tune the model with class weights\nhistory_cnn = model_cnn.fit(\n    train_generator,\n    epochs=epochs,\n    validation_data=validation_generator,\n    class_weight=class_weights\n)\n\n# Evaluate the model on the test set\nloss, accuracy = model_cnn.evaluate(test_generator)\nprint('Loss:', loss)\nprint(f'Test Accuracy: {accuracy * 100:.2f}%')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T06:41:57.068478Z","iopub.execute_input":"2024-11-29T06:41:57.068734Z","iopub.status.idle":"2024-11-29T06:45:03.963379Z","shell.execute_reply.started":"2024-11-29T06:41:57.068703Z","shell.execute_reply":"2024-11-29T06:45:03.962645Z"}},"outputs":[{"name":"stdout","text":"Found 1104 images belonging to 3 classes.\nFound 158 images belonging to 3 classes.\nFound 316 images belonging to 3 classes.\nTraining samples: 1104\nValidation samples: 158\nTest samples: 316\nClass Weights: {0: 0.5906902086677368, 1: 1.2474576271186442, 2: 1.978494623655914}\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 302ms/step - accuracy: 0.3373 - loss: 1.0733 - val_accuracy: 0.3861 - val_loss: 1.0230\nEpoch 2/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 205ms/step - accuracy: 0.4358 - loss: 1.0259 - val_accuracy: 0.6519 - val_loss: 0.9208\nEpoch 3/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 206ms/step - accuracy: 0.5382 - loss: 0.9953 - val_accuracy: 0.5506 - val_loss: 0.8821\nEpoch 4/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 206ms/step - accuracy: 0.5532 - loss: 0.9358 - val_accuracy: 0.5316 - val_loss: 0.8697\nEpoch 5/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 207ms/step - accuracy: 0.5667 - loss: 0.8545 - val_accuracy: 0.6329 - val_loss: 0.8007\nEpoch 6/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 209ms/step - accuracy: 0.5430 - loss: 0.8549 - val_accuracy: 0.6013 - val_loss: 0.8095\nEpoch 7/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 206ms/step - accuracy: 0.5753 - loss: 0.8029 - val_accuracy: 0.5696 - val_loss: 0.8480\nEpoch 8/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 204ms/step - accuracy: 0.6092 - loss: 0.7801 - val_accuracy: 0.6266 - val_loss: 0.7653\nEpoch 9/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 206ms/step - accuracy: 0.5696 - loss: 0.7752 - val_accuracy: 0.6456 - val_loss: 0.7621\nEpoch 10/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 208ms/step - accuracy: 0.5834 - loss: 0.7282 - val_accuracy: 0.6076 - val_loss: 0.7610\nEpoch 11/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 202ms/step - accuracy: 0.6078 - loss: 0.7352 - val_accuracy: 0.6013 - val_loss: 0.7348\nEpoch 12/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 203ms/step - accuracy: 0.6230 - loss: 0.7483 - val_accuracy: 0.7089 - val_loss: 0.6948\nEpoch 13/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 199ms/step - accuracy: 0.6204 - loss: 0.7285 - val_accuracy: 0.6582 - val_loss: 0.7054\nEpoch 14/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 203ms/step - accuracy: 0.6244 - loss: 0.7032 - val_accuracy: 0.6519 - val_loss: 0.6942\nEpoch 15/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 207ms/step - accuracy: 0.6288 - loss: 0.7135 - val_accuracy: 0.6329 - val_loss: 0.6918\nEpoch 16/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 205ms/step - accuracy: 0.6360 - loss: 0.7272 - val_accuracy: 0.6899 - val_loss: 0.6672\nEpoch 17/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 204ms/step - accuracy: 0.6445 - loss: 0.6907 - val_accuracy: 0.7152 - val_loss: 0.6587\nEpoch 18/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 200ms/step - accuracy: 0.6435 - loss: 0.7032 - val_accuracy: 0.6899 - val_loss: 0.6693\nEpoch 19/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 201ms/step - accuracy: 0.6397 - loss: 0.6841 - val_accuracy: 0.7025 - val_loss: 0.6701\nEpoch 20/20\n\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 200ms/step - accuracy: 0.6721 - loss: 0.6861 - val_accuracy: 0.6203 - val_loss: 0.7086\n\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 186ms/step - accuracy: 0.5405 - loss: 0.7900\nLoss: 0.7210988402366638\nTest Accuracy: 60.13%\n","output_type":"stream"}],"execution_count":6}]}